{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "fanout_final_test_w_content = \"/vepfs/wcf/G/zecheng/data/fanout-final-test-content.json\"\n",
    "content = auto_read_data(fanout_final_test_w_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Fanout(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_context_length) -> None:\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.content = auto_read_data(file_path)\n",
    "        self.TEMPLATE = \"*** BEGIN DATA ***\\n\\n{context}\\n*** END DATA ***\\n\\n \\\n",
    "            Answer the following question based on the documents above, and output only your answer. \\\n",
    "            If the answer is a list, output one on each line. \\n\\n[Question]: {question}\"\n",
    "        self.QUESTION_TEMPLATE = \"<document>\\n<title>{title}</title>\\n<content>{evidence}</content>\\n</document>\\n\"\n",
    "        self.max_context_length = max_context_length\n",
    "\n",
    "    def __len__(self, index) -> Any:\n",
    "        return len(self.content)\n",
    "\n",
    "    def filter_tok_context_length(self, s, L) -> str:\n",
    "        tok_context = self.tokenizer(s, return_tensors='pt', add_special_tokens=False).input_ids[:L]\n",
    "        decoded_context = self.tokenizer.decode(tok_context[0], skip_special_tokens=True)\n",
    "        return decoded_context\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        sample = self.content[index]\n",
    "        titles = [item['title'] for item in sample]\n",
    "        evidences = [item['content'] for item in sample]\n",
    "        # cut the evidence length to fit the input length\n",
    "\n",
    "        num_evidences = len(evidences)\n",
    "        per_evidence_max_length = self.max_context_length // num_evidences\n",
    "        evidences = [self.filter_tok_context_length(s, per_evidence_max_length) for s in evidences]\n",
    "        context = [self.QUESTION_TEMPLATE.format(title, evidence) for title, evidence in zip(titles, evidences)]\n",
    "        query = self.TEMPLATE.format(context=context, question=sample['question'])\n",
    "\n",
    "        # your tokenizer operation\n",
    "        tok_query = self.tokenizer(query, return_tensors='pt')\n",
    "\n",
    "        return tok_query  # including input_ids and attention mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
