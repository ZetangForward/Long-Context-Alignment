{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这里直接将Spe Token替换成英文的句号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/rlhf_flow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\u001b[4m\u001b[36mModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-07-24 15:01:30\u001b[0m\n"
=======
      "\u001b[4m\u001b[36mModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-07-12 17:35:31\u001b[0m\n"
>>>>>>> c02f2140e0cd59321df93afc3afc942a5837572e
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, load_from_disk, DatasetDict\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from contextlib import contextmanager, nullcontext\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Union, Literal, Dict\n",
    "from modelzipper.tutils import *\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from scipy.stats import qmc\n",
    "import numpy as np\n",
    "import itertools\n",
    "from functools import partial\n",
    "from accelerate import PartialState\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create POSE and Padding Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3, 4]), tensor([3, 4, 5, 6, 7]), tensor([ 7,  8,  9, 10, 11])]\n"
     ]
    }
   ],
   "source": [
    "def find_index(id_lst, prefix_id, suffix_id):\n",
    "    return id_lst.index(prefix_id), id_lst.index(suffix_id)\n",
    "\n",
    "def create_position_ids(N, L):\n",
    "    \"\"\"sampling N points from L (max_chunk_size space)\"\"\"\n",
    "    if N == L:\n",
    "        start_pos = 0\n",
    "    else:\n",
    "        start_pos = np.random.randint(0, L - N)\n",
    "    end_pos = start_pos + N\n",
    "    position_ids = torch.arange(start_pos, end_pos)\n",
    "    return position_ids\n",
    "\n",
    "def create_covering_position_ids(N, L):\n",
    "    \"\"\"Create sets of position IDs to cover all positions from 0 to L-1 with intervals of length N.\"\"\"\n",
    "    if N > L:\n",
    "        raise ValueError(\"N should not be greater than L\")\n",
    "    num_intervals = (L + N - 1) // N\n",
    "    position_ids_list = []\n",
    "    for i in range(num_intervals):\n",
    "        start_pos = i * (L - N) // (num_intervals - 1) if num_intervals > 1 else 0\n",
    "        end_pos = start_pos + N\n",
    "        if end_pos > L:\n",
    "            end_pos = L\n",
    "            start_pos = L - N if L > N else 0\n",
    "        position_ids = torch.arange(start_pos, end_pos)\n",
    "        position_ids_list.append(position_ids)\n",
    "    return position_ids_list\n",
    "\n",
    "def auto_padding(t: torch.Tensor, length: int, filling_value=-100, return_attention_mask=False):\n",
    "    if length < t.size(0):\n",
    "        if return_attention_mask: \n",
    "            return t[:length]\n",
    "        else:                                   \n",
    "            return t[:length], torch.ones_like(t[:length])\n",
    "    padded_tensor = torch.full((length,), filling_value, dtype=t.dtype)\n",
    "    padded_tensor[:t.size(0)] = t\n",
    "    if return_attention_mask:\n",
    "        attention_mask = torch.zeros(length, dtype=torch.int)\n",
    "        attention_mask[:t.size(0)] = 1\n",
    "        return padded_tensor, attention_mask\n",
    "    return padded_tensor\n",
    "\n",
    "# 使用示例\n",
    "N = 5\n",
    "L = 12\n",
    "\n",
    "position_ids_list = create_covering_position_ids(N, L)\n",
    "print(position_ids_list)\n",
    "# for idx, pos_ids in enumerate(position_ids_list):\n",
    "#     print(f\"Position IDs {idx+1}:\", pos_ids)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test Create QA Function'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_fn(lst, max_candidates=2, max_combination=16):\n",
    "    trimmed_lists = [random.sample(sublst, min(len(sublst), max_candidates)) for sublst in lst]\n",
    "    all_combinations = list(itertools.islice(itertools.product(*trimmed_lists), max_combination))\n",
    "    random.shuffle(all_combinations)\n",
    "    return [torch.cat(combination) for combination in all_combinations[:max_combination]]\n",
    "\n",
    "def create_system_suffix(tokenizer, system_suffix, special_token_id: int=13):\n",
    "    tok_suffix = tokenizer(system_suffix, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    padded_tok_suffix, padded_suffix_attention_mask = tok_suffix.input_ids[0], tok_suffix.attention_mask[0]\n",
    "    # add special token\n",
    "    padded_tok_suffix = torch.concatenate([padded_tok_suffix, torch.tensor([special_token_id])], dim=0)\n",
    "    padded_suffix_attention_mask = torch.concatenate([padded_suffix_attention_mask, torch.tensor([1])], dim=0)\n",
    "    system_position_ids = torch.arange(0, padded_tok_suffix.size(-1))\n",
    "    return padded_tok_suffix, padded_suffix_attention_mask, system_position_ids\n",
    "\n",
    "\n",
    "def cut_length(s: str, max_length: int, tokenizer: AutoTokenizer):\n",
    "    \"\"\"Cut the string to the maximum length allowed by the tokenizer.\"\"\"\n",
    "    input_ids = tokenizer(s, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    if input_ids.size(-1) > max_length:\n",
    "        input_ids = input_ids[:max_length]\n",
    "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def create_ref_chunk(tokenizer: AutoTokenizer, all_refs: List[str], real_reference_size: int, max_embedding_size: int, system_prompt_size: int, qa_size: int, special_token_id: int=13, add_big_chunk: bool = False):\n",
    "    # 创建 Sparse Chunk 的位置信息，padded的内容， attention_mask, 以及所有的special token的位置\n",
    "    sparse_chunk_pos_candicated_padded_position_ids, padded_ref_input_ids, padded_ref_attention_mask, _ = create_sparse_chunked_reference(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, special_token_id)\n",
    "\n",
    "    # 创建 Continuous Chunk 的位置信息，padded的内容， attention_mask, 以及所有的special token的位置\n",
    "    if add_big_chunk:\n",
    "        candicated_padded_position_ids, padded_big_chunk_tok, padded_big_chunk_attention_mask = create_big_chunked_reference(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, special_token_id)\n",
    "\n",
    "        return {\n",
    "            \"sparse_ref_chunk\": (sparse_chunk_pos_candicated_padded_position_ids, padded_ref_input_ids, padded_ref_attention_mask),\n",
    "            \"big_ref_chunk\": (candicated_padded_position_ids, padded_big_chunk_tok, padded_big_chunk_attention_mask)\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"sparse_ref_chunk\": (sparse_chunk_pos_candicated_padded_position_ids, padded_ref_input_ids, padded_ref_attention_mask),\n",
    "    }\n",
    "\n",
    "\n",
    "def create_big_chunked_reference(tokenizer: AutoTokenizer, all_refs: List[str], real_reference_size: int, max_embedding_size: int, system_prompt_size: int, qa_size: int, special_token_id: int=13):\n",
    "    \"\"\"这里是写连续真实的chunk size\"\"\"\n",
    "    special_token_str = tokenizer.decode(special_token_id)[:-1]\n",
    "    big_chunk_str = f\" {special_token_str} \".join(all_refs)\n",
    "    big_chunk_str = cut_length(big_chunk_str, real_reference_size, tokenizer)\n",
    "    big_chunk_tok = tokenizer(big_chunk_str, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    padded_big_chunk_tok, padded_big_chunk_attention_mask = auto_padding(big_chunk_tok, real_reference_size, filling_value=0, return_attention_mask=True)\n",
    "\n",
    "    # fake_position_chunk_size = (max_embedding_size - qa_size - system_prompt_size) // real_reference_size\n",
    "    positional_chunks = torch.arange(system_prompt_size, max_embedding_size - qa_size, padded_big_chunk_attention_mask.sum())\n",
    "    candicated_padded_position_ids = []\n",
    "\n",
    "    for begin_pos in positional_chunks:\n",
    "        if begin_pos + padded_big_chunk_attention_mask.sum() > max_embedding_size - qa_size:\n",
    "            break  # 最后一个位置超过了最大的max embedding size\n",
    "        position_ids = torch.arange(begin_pos, begin_pos + padded_big_chunk_attention_mask.sum())\n",
    "        padded_position_ids = auto_padding(position_ids, real_reference_size, filling_value=0, return_attention_mask=False)\n",
    "        candicated_padded_position_ids.append(padded_position_ids)\n",
    "    \n",
    "    return candicated_padded_position_ids, padded_big_chunk_tok, padded_big_chunk_attention_mask\n",
    "\n",
    "\n",
    "def create_sparse_chunked_reference(tokenizer: AutoTokenizer, all_refs: List[str], real_reference_size: int, max_embedding_size: int, system_prompt_size: int, qa_size: int, special_token_id: int=13):\n",
    "    real_max_chunk_size = real_reference_size // len(all_refs) - 1 # allocate one position for special token\n",
    "    tok_all_ref = [tokenizer(item, return_tensors=\"pt\", add_special_tokens=False).input_ids[0] for item in all_refs]\n",
    "    truncted_refer_tok_lst, statistic_data_size = [], []\n",
    "    for item in tok_all_ref:\n",
    "        statistic_data_size.append(item.size(-1))\n",
    "        if item.size(-1) > real_max_chunk_size: \n",
    "            item = item[: real_max_chunk_size]\n",
    "        truncted_refer_tok_lst.append(item)\n",
    "\n",
    "    fake_position_chunk_size = (max_embedding_size - qa_size - system_prompt_size) // len(tok_all_ref)\n",
    "    positional_chunks = torch.arange(system_prompt_size, max_embedding_size - qa_size, fake_position_chunk_size)\n",
    "    # Here, end_positional_chunks denotes special token ids\n",
    "    begin_positional_chunks, end_positional_chunks = positional_chunks[:-1], positional_chunks[1:] - 1 # 每个chunk的起始位置和结束位置（均包含这些位置编码id） \n",
    "    all_chunk_pos_lst = []\n",
    "    \n",
    "    for i, item in enumerate(truncted_refer_tok_lst):\n",
    "        chunk_token_pos_lst = create_covering_position_ids(item.size(-1), fake_position_chunk_size-1)\n",
    "        chunk_token_pos_lst = [item + begin_positional_chunks[i] for item in chunk_token_pos_lst]\n",
    "        all_chunk_pos_lst.append(chunk_token_pos_lst)\n",
    "\n",
    "    padded_chunk_pos_lst = [[auto_padding(sub_item, real_max_chunk_size, filling_value=0, return_attention_mask=False) for sub_item in item] for item in all_chunk_pos_lst]\n",
    "    padded_refer_tok_lst = [auto_padding(item, real_max_chunk_size, filling_value=0, return_attention_mask=True) for item in truncted_refer_tok_lst]\n",
    "    padded_refer_tok_ids = [item[0] for item in padded_refer_tok_lst]\n",
    "    padded_refer_attention_mask = [item[1] for item in padded_refer_tok_lst]\n",
    "\n",
    "    candicated_padded_position_ids = []\n",
    "    padded_ref_input_ids_lst, padded_ref_attention_mask_lst = [], []\n",
    "    \n",
    "    for chunk_pos_ids, chunk_spe_pos_lst in zip(end_positional_chunks, padded_chunk_pos_lst):\n",
    "        tmp_chunk_pos_ids = []\n",
    "        for tmp in chunk_spe_pos_lst:\n",
    "            tmp = torch.concatenate([tmp, torch.tensor([chunk_pos_ids])], dim=0)\n",
    "            tmp_chunk_pos_ids.append(tmp)  # [[0,1,...,C1], [C2,C2+1,...,C3], ...]\n",
    "        candicated_padded_position_ids.append(tmp_chunk_pos_ids)\n",
    "    candicated_padded_position_ids = combine_fn(candicated_padded_position_ids, max_combination=8)\n",
    "\n",
    "    for padded_chunk_tok_ref_input_ids, padded_chunk_tok_ref_attention_mask in zip(padded_refer_tok_ids, padded_refer_attention_mask):\n",
    "        padded_chunk_tok_ref_input_ids = torch.concatenate([padded_chunk_tok_ref_input_ids, torch.tensor([special_token_id])], dim=0)\n",
    "        padded_chunk_tok_ref_attention_mask = torch.concatenate([padded_chunk_tok_ref_attention_mask, torch.tensor([1])], dim=0)\n",
    "        padded_ref_input_ids_lst.append(padded_chunk_tok_ref_input_ids)\n",
    "        padded_ref_attention_mask_lst.append(padded_chunk_tok_ref_attention_mask)\n",
    "    \n",
    "    padded_ref_input_ids = torch.concatenate(padded_ref_input_ids_lst, dim=0)\n",
    "    padded_ref_attention_mask = torch.concatenate(padded_ref_attention_mask_lst, dim=0)\n",
    "    all_spe_pos = torch.arange(real_max_chunk_size, real_reference_size, real_max_chunk_size + 1)\n",
    "\n",
    "    return candicated_padded_position_ids, padded_ref_input_ids, padded_ref_attention_mask, all_spe_pos\n",
    "\n",
    "\n",
    "def create_qa(QUESTION_TEMPLATE, ANSWER_TEMPLATE, combined_question, combined_answer, prefix_a: str, suffix_a: str, last_position: int, qa_size: int, special_token_id: int):\n",
    "    \"\"\"\n",
    "    last_position 是reference position ids最大的数值，下面的代码要加一个 last_position + 1的shift\n",
    "    qa_size 规定了最大的qa 的长度，所以总长度需要手动卡一下\n",
    "    \"\"\"\n",
    "    # Create Question\n",
    "    question = QUESTION_TEMPLATE.format(question=combined_question)\n",
    "    tok_question = tokenizer(question, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    padded_tok_question, padded_question_attention_mask = auto_padding(tok_question, tok_question.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    padded_tok_question = torch.concatenate([padded_tok_question, torch.tensor([special_token_id])], dim=0)\n",
    "    padded_question_attention_mask = torch.concatenate([padded_question_attention_mask, torch.tensor([1])], dim=0)\n",
    "    question_position_input_ids = create_position_ids(tok_question.size(-1), tok_question.size(-1)) + last_position + 1\n",
    "    last_pos = question_position_input_ids.max() + 1\n",
    "    question_position_input_ids = torch.concatenate([question_position_input_ids, torch.tensor([last_pos])], dim=0)\n",
    "    spe_tok_pos = question_position_input_ids.size(-1) - 1\n",
    "\n",
    "    # Create Chosen / Rejected Answers / and their labels\n",
    "    chosen_answer = ANSWER_TEMPLATE.format(answer=combined_answer)\n",
    "    prefix_rejected_answer = ANSWER_TEMPLATE.format(answer=prefix_a)\n",
    "    suffix_rejected_answer = ANSWER_TEMPLATE.format(answer=suffix_a)\n",
    "    tok_chosen_answer = tokenizer(chosen_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    tok_prefix_rejected_answer = tokenizer(prefix_rejected_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    tok_suffix_rejected_answer = tokenizer(suffix_rejected_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    system_reference_question_size = last_position + 1 + question_position_input_ids.size(-1)\n",
    "\n",
    "    padded_tok_chosen_answer, padded_chosen_answer_attention_mask = auto_padding(tok_chosen_answer, qa_size-padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_chosen_answer_labels = auto_padding(tok_chosen_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    chosen_answer_position_ids = create_position_ids(tok_chosen_answer.size(-1), tok_chosen_answer.size(-1)) + system_reference_question_size\n",
    "    chosen_answer_position_ids = auto_padding(chosen_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "\n",
    "    padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_prefix_rejected_answer_labels = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    prefix_rejected_answer_position_ids = create_position_ids(tok_prefix_rejected_answer.size(-1), tok_prefix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    prefix_rejected_answer_position_ids = auto_padding(prefix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "    \n",
    "    padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_suffix_rejected_answer_labels = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    suffix_rejected_answer_position_ids = create_position_ids(tok_suffix_rejected_answer.size(-1), tok_suffix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    suffix_rejected_answer_position_ids = auto_padding(suffix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    " \n",
    "    return padded_tok_question, padded_question_attention_mask, question_position_input_ids, \\\n",
    "        padded_tok_chosen_answer, padded_chosen_answer_attention_mask, tok_chosen_answer_labels, chosen_answer_position_ids, \\\n",
    "        padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, prefix_rejected_answer_position_ids, \\\n",
    "        padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, suffix_rejected_answer_position_ids, spe_tok_pos\n",
    "    \n",
    "\n",
    "\"\"\"block testing create_chunked_reference\"\"\" \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/data/zecheng/hf_models/Meta-Llama-3-8B-Instruct\")\n",
    "all_refs = [\"hello, world\", \"Any, iowpq\", \"reason medsa\"]\n",
    "real_reference_size = 28\n",
    "system_prompt_size, qa_size = 2, 4\n",
    "max_embedding_size = 3400\n",
    "res = create_ref_chunk(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, add_big_chunk=True)\n",
    "sparse_candicated_padded_position_ids, sparse_padded_ref_input_ids, sparse_padded_attention_mask = res[\"sparse_ref_chunk\"]\n",
    "big_candicated_padded_position_ids, big_padded_ref_input_ids, big_padded_attention_mask = res[\"big_ref_chunk\"]\n",
    "\n",
    "\"\"\"Test Create QA Function\"\"\"\n",
    "# QUESTION_TEMPLATE = \"<|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the following question according to the references: {question}<|eot_id|>\"\n",
    "# ANSWER_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\\n\\nThe answer is: {answer}<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "# padded_tok_question, padded_question_attention_mask, \\\n",
    "# question_position_input_ids, padded_tok_chosen_answer, \\\n",
    "# padded_chosen_answer_attention_mask, tok_chosen_answer_labels, \\\n",
    "# chosen_answer_position_ids, padded_tok_prefix_rejected_answer, \\\n",
    "# padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, \\\n",
    "# prefix_rejected_answer_position_ids, padded_tok_suffix_rejected_answer, \\\n",
    "# padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, \\\n",
    "# suffix_rejected_answer_position_ids, spe_tok_pos = create_qa(QUESTION_TEMPLATE, ANSWER_TEMPLATE, \"who are you\", \"jack\", \"prefix_a\", \"suffix_a\", last_position=1024, qa_size=100, special_token_id=13)\n",
    "\n",
    "# res = create_ref_chunk(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, add_big_chunk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Current Avg Seq Length: 10308.86:  64%|██████▍   | 238/371 [01:05<00:36,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Avg Seq Length: 16052.81: 100%|██████████| 878/878 [01:46<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current Avg Seq Length: 16051.26: 100%|██████████| 1032/1032 [02:26<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_covering_position_ipt_data(tokenizer, all_refs: List[str], combined_question: str, combined_answer: str, prefix_a: str, suffix_a: str, qa_size: int, max_embedding_size: int, real_reference_size: int, special_token_id: int = None, prefix_id: int = None, suffix_id: int = None):\n",
    "    statistic_data_size = []\n",
    "\n",
    "    SYSTEM_SUFFIX = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nBelow is some references. Please read it carefully and answer the following question.<|eot_id|>\"\n",
    "    QUESTION_TEMPLATE = \"<|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the following question according to the references: {question}<|eot_id|>\"\n",
    "    ANSWER_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\\n\\nThe answer is: {answer}<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "    # Create System Suffix\n",
    "    padded_tok_input_ids_system_suffix, padded_attention_mask_system_suffix, padded_position_ids_system_suffix = create_system_suffix(tokenizer, SYSTEM_SUFFIX, special_token_id)\n",
    "    system_prompt_size = padded_attention_mask_system_suffix.size(-1)\n",
    " \n",
    "    # create chunk reference (input_ids, attention_mask and positional ids)\n",
    "    res = create_ref_chunk(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, special_token_id, add_big_chunk=True)\n",
    "\n",
    "    candicated_padded_position_ids_lst, padded_ref_input_ids, padded_ref_attention_mask = res[\"sparse_ref_chunk\"]\n",
    "    global_candicated_padded_position_ids_lst, global_padded_ref_input_ids, global_padded_ref_attention_mask = res[\"big_ref_chunk\"]\n",
    "\n",
    "    # combine and wrap each position_id, input_ids and attention_mask\n",
    "    last_position = max_embedding_size - qa_size  # size for real reference and system prompt\n",
    "\n",
    "    # Create Question, all Answers\n",
    "    padded_tok_question, padded_question_attention_mask, \\\n",
    "    question_position_input_ids, padded_tok_chosen_answer, \\\n",
    "    padded_chosen_answer_attention_mask, tok_chosen_answer_labels, \\\n",
    "    chosen_answer_position_ids, padded_tok_prefix_rejected_answer, \\\n",
    "    padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, \\\n",
    "    prefix_rejected_answer_position_ids, padded_tok_suffix_rejected_answer, \\\n",
    "    padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, \\\n",
    "    suffix_rejected_answer_position_ids, _ = create_qa(\n",
    "        QUESTION_TEMPLATE, ANSWER_TEMPLATE, combined_question, combined_answer, prefix_a, suffix_a, last_position, qa_size, special_token_id=special_token_id\n",
    "    )\n",
    "    sparse_datasets, global_datasets = [], []  # different combination of positions \n",
    "\n",
    "    # create sparse input data\n",
    "    for ref_position_id in candicated_padded_position_ids_lst:\n",
    "        concatenated_batch = {}\n",
    "        concatenated_batch[\"input_ids\"] = torch.concatenate([padded_tok_input_ids_system_suffix, padded_ref_input_ids, padded_tok_question], dim=0)\n",
    "        concatenated_batch[\"attention_mask\"] = torch.concatenate([padded_attention_mask_system_suffix, padded_ref_attention_mask, padded_question_attention_mask], dim=0)\n",
    "        concatenated_batch[\"position_ids\"] = torch.concatenate([padded_position_ids_system_suffix, ref_position_id, question_position_input_ids], dim=0)\n",
    "        referece_question_length = concatenated_batch[\"attention_mask\"].size(-1)\n",
    "        referece_question_labels = torch.full((1, referece_question_length), -100)[0]\n",
    "        \n",
    "        # Create Labels for Each Part\n",
    "        concatenated_batch[\"chosen_answer\"] = {\n",
    "            \"input_ids\": padded_tok_chosen_answer, \n",
    "            \"attention_mask\": padded_chosen_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_chosen_answer_labels], dim=0),\n",
    "            \"position_ids\": chosen_answer_position_ids\n",
    "        }\n",
    "        concatenated_batch[\"prefix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_prefix_rejected_answer, \n",
    "            \"attention_mask\": padded_prefix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_prefix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": prefix_rejected_answer_position_ids,\n",
    "        }\n",
    "        concatenated_batch[\"suffix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_suffix_rejected_answer, \n",
    "            \"attention_mask\": padded_suffix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_suffix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": suffix_rejected_answer_position_ids,\n",
    "        }\n",
    "        if prefix_id is not None and suffix_id is not None:\n",
    "            concatenated_batch[\"chosen_ids\"] = (prefix_id, suffix_id)\n",
    "        sparse_datasets.append(concatenated_batch)\n",
    "        statistic_data_size.append(concatenated_batch[\"input_ids\"].size(-1))\n",
    "\n",
    "    # create sparse input data\n",
    "    for ref_position_id in global_candicated_padded_position_ids_lst:\n",
    "        concatenated_batch = {}\n",
    "        concatenated_batch[\"input_ids\"] = torch.concatenate([padded_tok_input_ids_system_suffix, global_padded_ref_input_ids, padded_tok_question], dim=0)\n",
    "        concatenated_batch[\"attention_mask\"] = torch.concatenate([padded_attention_mask_system_suffix, global_padded_ref_attention_mask, padded_question_attention_mask], dim=0)\n",
    "        concatenated_batch[\"position_ids\"] = torch.concatenate([padded_position_ids_system_suffix, ref_position_id, question_position_input_ids], dim=0)\n",
    "        referece_question_length = concatenated_batch[\"attention_mask\"].size(-1)\n",
    "        referece_question_labels = torch.full((1, referece_question_length), -100)[0]\n",
    "        \n",
    "        # Create Labels for Each Part\n",
    "        concatenated_batch[\"chosen_answer\"] = {\n",
    "            \"input_ids\": padded_tok_chosen_answer, \n",
    "            \"attention_mask\": padded_chosen_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_chosen_answer_labels], dim=0),\n",
    "            \"position_ids\": chosen_answer_position_ids\n",
    "        }\n",
    "        concatenated_batch[\"prefix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_prefix_rejected_answer, \n",
    "            \"attention_mask\": padded_prefix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_prefix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": prefix_rejected_answer_position_ids,\n",
    "        }\n",
    "        concatenated_batch[\"suffix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_suffix_rejected_answer, \n",
    "            \"attention_mask\": padded_suffix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_suffix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": suffix_rejected_answer_position_ids,\n",
    "        }\n",
    "        if prefix_id is not None and suffix_id is not None:\n",
    "            concatenated_batch[\"chosen_ids\"] = (prefix_id, suffix_id)\n",
    "        global_datasets.append(concatenated_batch)\n",
    "        statistic_data_size.append(concatenated_batch[\"input_ids\"].size(-1))\n",
    "\n",
    "    merged_datasets = sparse_datasets + global_datasets\n",
    "\n",
    "    return merged_datasets, sum(statistic_data_size) / len(merged_datasets)\n",
    "\n",
    "\n",
    "def process_item(item):\n",
    "    # 在这个函数中，我们不使用 tokenizer\n",
    "    all_ref_text = item[\"all_ref_text\"]\n",
    "    combined_question, final_answer = item[\"combined_question\"], item[\"final_answer\"]\n",
    "    prefix_a, suffix_a = item[\"prefix_a\"], item[\"suffix_a\"]\n",
    "    prefix_id, suffix_id = None, None\n",
    "\n",
    "    # 注意：这里我们传递文本，而不是使用 tokenizer\n",
    "    return (all_ref_text, combined_question, final_answer, prefix_a, suffix_a, prefix_id, suffix_id)\n",
    "\n",
    "def create_datasets(args):\n",
    "    all_ref_text, combined_question, final_answer, prefix_a, suffix_a, prefix_id, suffix_id = args\n",
    "    all_datasets, ref_length = create_covering_position_ipt_data(tokenizer, all_ref_text, combined_question, final_answer, prefix_a, suffix_a, qa_size=256, max_embedding_size=65536, real_reference_size=16384, special_token_id=spe_token_id, prefix_id=prefix_id, suffix_id=suffix_id)\n",
    "    return all_datasets, ref_length\n",
    "\n",
    "def map_fn(item, tokenizer, special_token_id, qa_size, max_embedding_size, real_reference_size, prefix_id, suffix_id):\n",
    " \n",
    "    all_ref_text = item[\"all_ref_text\"]\n",
    "    combined_question, final_answer = item[\"combined_question\"], item[\"final_answer\"]\n",
    "    prefix_q, suffix_q = item[\"prefix_q\"], item[\"suffix_q\"]\n",
    "    prefix_a, suffix_a = item[\"prefix_a\"], item[\"suffix_a\"]\n",
    "    prefix_id, suffix_id = None, None\n",
    "    all_datasets, ref_length = create_covering_position_ipt_data(tokenizer, all_ref_text, combined_question, final_answer, prefix_a, suffix_a, qa_size=qa_size, max_embedding_size=max_embedding_size, real_reference_size=real_reference_size, special_token_id=special_token_id, prefix_id=prefix_id, suffix_id=suffix_id)\n",
    "    result_dict = {f'column_{i}': all_datasets[i] for i in range(len(all_datasets))}\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "dataset_chunk16 = datasets.load_from_disk(\"/data/zecheng/data/processed_project/16chunk/1024_chunk_size/hf_data_step3\")\n",
    "dataset2_chunk_4_8 = datasets.load_from_disk(\"/data/zecheng/data/process_wiki_document/two_hop/hf_dataset_step2\")\n",
    "dataset3_chunk_2_2 = datasets.load_from_disk(\"/data/zecheng/data/processed_project/mix_chunks_v3/aug_split\")\n",
    "# dataset3_chunk_2 = datasets.load_from_disk(\"/data/zecheng/data/process_wiki_document/two_hop/hf_dataset_step3\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/data/zecheng/hf_models/Meta-Llama-3-8B-Instruct\")\n",
    "training_samples = []\n",
    "spe_token_id = tokenizer(\".\", add_special_tokens=False).input_ids[-1]\n",
    "\n",
    "# processed_data = dataset.map(map_fn, fn_kwargs={\"tokenizer\": tokenizer, \"special_token_id\": spe_token_id, \"qa_size\": 256, \"max_embedding_size\": 65536, \"real_reference_size\": 8192, \"prefix_id\": None, \"suffix_id\": None}, num_proc=16)\n",
    "all_datassets = []\n",
    "for dataset in [dataset3_chunk_2_2, dataset2_chunk_4_8, dataset_chunk16]:\n",
    "    avg_real_seq_length = 0\n",
    "    with tqdm(total=len(dataset), desc=f\"Initial Avg Length: {avg_real_seq_length}\") as pbar:\n",
    "        for item in dataset:\n",
    "            try:\n",
    "                all_ref_text = item[\"all_ref_text\"]\n",
    "                combined_question, final_answer = item[\"combined_question\"], item[\"final_answer\"]\n",
    "                prefix_q, suffix_q = item[\"prefix_q\"], item[\"suffix_q\"]\n",
    "                prefix_a, suffix_a = item[\"prefix_a\"], item[\"suffix_a\"]\n",
    "                prefix_id, suffix_id = None, None\n",
    "                all_datasets, ref_length = create_covering_position_ipt_data(tokenizer, all_ref_text, combined_question, final_answer, prefix_a, suffix_a, qa_size=256, max_embedding_size=65536, real_reference_size=16000, special_token_id=spe_token_id, prefix_id=prefix_id, suffix_id=suffix_id)\n",
    "                avg_real_seq_length += ref_length / len(dataset)\n",
    "                training_samples.extend(all_datasets)\n",
    "            except:\n",
    "                print(f\"Error\")\n",
    "                continue\n",
    "            pbar.set_description(f\"Current Avg Seq Length: {avg_real_seq_length:.2f}\")\n",
    "            pbar.update(1)\n",
    "    print(len(training_samples))\n",
    "    all_datassets.extend(training_samples)\n",
    "\n",
    "random.shuffle(all_datassets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16045])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples[1]['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([128000, 128006,   9125,  ...,     30, 128009,     13]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]),\n",
       " 'position_ids': tensor([    0,     1,     2,  ..., 65307, 65308, 65309]),\n",
       " 'chosen_answer': {'input_ids': tensor([128006,  78191, 128007,    271,    791,   4320,    374,     25,  27193,\n",
       "           58917, 128009, 128001,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  'labels': tensor([-100, -100, -100,  ..., -100, -100, -100]),\n",
       "  'position_ids': tensor([65310, 65311, 65312, 65313, 65314, 65315, 65316, 65317, 65318, 65319,\n",
       "          65320, 65321,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0])},\n",
       " 'prefix_rejected_answer': {'input_ids': tensor([128006,  78191, 128007,    271,    791,   4320,    374,     25,   5473,\n",
       "            4469,   5023,   1232,    364,   7440,  48194,    323,  18539,    527,\n",
       "            1403,    315,    279,   1455,   3062,  19476,    304,   4332,   6067,\n",
       "              13,   7440,  48194,  26420,    430,    682,   7954,    304,    279,\n",
       "            1887,    617,    279,   1890,   1684,    315,    279,    828,    520,\n",
       "             682,   3115,     13,  52910,  26420,    430,    279,   1887,    374,\n",
       "            2744,  15987,    323,    649,   1920,   7540,   7255,     77,   1734,\n",
       "             644,   4332,   6067,     11,    279,   6696,   1885,   1990,  29237,\n",
       "             323,  18539,    374,   7633,    555,    279,  27193,  58917,     13,\n",
       "             578,  27193,  58917,   5415,    430,    264,   4332,   1887,    649,\n",
       "             617,    520,   1455,   1403,    704,    315,    279,   2380,   2768,\n",
       "            6012,   7338,     77,   1734,      9,   7440,  48194,     25,   2052,\n",
       "            7954,    304,    279,   1887,    617,    279,   1890,   1684,    315,\n",
       "             279,    828,    520,    682,   3115,   7255,     77,      9,  52910,\n",
       "              25,    578,   1887,    374,   2744,  15987,    323,    649,   1920,\n",
       "            7540,   7255,     77,      9,  55726,    350,  32761,     25,    578,\n",
       "            1887,    649,   3136,    518,    364,   5514,    851,   1232,    220,\n",
       "              16,     92, 128009, 128001,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  'labels': tensor([-100, -100, -100,  ..., -100, -100, -100]),\n",
       "  'position_ids': tensor([65310, 65311, 65312, 65313, 65314, 65315, 65316, 65317, 65318, 65319,\n",
       "          65320, 65321, 65322, 65323, 65324, 65325, 65326, 65327, 65328, 65329,\n",
       "          65330, 65331, 65332, 65333, 65334, 65335, 65336, 65337, 65338, 65339,\n",
       "          65340, 65341, 65342, 65343, 65344, 65345, 65346, 65347, 65348, 65349,\n",
       "          65350, 65351, 65352, 65353, 65354, 65355, 65356, 65357, 65358, 65359,\n",
       "          65360, 65361, 65362, 65363, 65364, 65365, 65366, 65367, 65368, 65369,\n",
       "          65370, 65371, 65372, 65373, 65374, 65375, 65376, 65377, 65378, 65379,\n",
       "          65380, 65381, 65382, 65383, 65384, 65385, 65386, 65387, 65388, 65389,\n",
       "          65390, 65391, 65392, 65393, 65394, 65395, 65396, 65397, 65398, 65399,\n",
       "          65400, 65401, 65402, 65403, 65404, 65405, 65406, 65407, 65408, 65409,\n",
       "          65410, 65411, 65412, 65413, 65414, 65415, 65416, 65417, 65418, 65419,\n",
       "          65420, 65421, 65422, 65423, 65424, 65425, 65426, 65427, 65428, 65429,\n",
       "          65430, 65431, 65432, 65433, 65434, 65435, 65436, 65437, 65438, 65439,\n",
       "          65440, 65441, 65442, 65443, 65444, 65445, 65446, 65447, 65448, 65449,\n",
       "          65450, 65451, 65452, 65453, 65454, 65455, 65456, 65457, 65458, 65459,\n",
       "          65460, 65461, 65462, 65463, 65464, 65465, 65466,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0])},\n",
       " 'suffix_rejected_answer': {'input_ids': tensor([128006,  78191, 128007,    271,    791,   4320,    374,     25,   5473,\n",
       "            4469,   5023,   1232,    364,  40665,    264,  10015,  12399,    315,\n",
       "             279,  27193,  58917,     13,   2650,   1587,    279,   7434,    315,\n",
       "             330,  54162,      1,   1782,    505,   1202,   4279,   7419,  33720,\n",
       "              77,   1734,    827,  45178,   7338,     77,   1734,      9,    510,\n",
       "              33,   4361,    261,     11,    469,     13,    320,   1049,     15,\n",
       "            7400,    330,  92459,  22514,   4332,   6067,    320,  16647,  95110,\n",
       "           55227,    315,    279,    220,    777,    339,  25992,  90315,  74938,\n",
       "             389,  58014,    315,  45055,  46879,     11,    220,     22,   4235,\n",
       "             717,   7255,     77,      9,    510,     43,  69581,     11,    452,\n",
       "              13,    362,   2637,    612,  46092,     11,    328,     13,    320,\n",
       "            1049,     17,   7400,    330,     33,   4361,    261,  10379,     82,\n",
       "           87015,    554,    323,    279,  69543,    315,  13263,     11,   2561,\n",
       "              11,    323,  17071,   2442,  22847,    519,  21248,    509,  25480,\n",
       "            1734,      9,    510,     47,  92689,   7211,     11,    350,     13,\n",
       "             320,    679,     16,   7400,    330,  10445,  31006,   5829,  82342,\n",
       "            3343,    507,  10379,    697,    518,    364,   5514,    851,   1232,\n",
       "             220,     15,     92, 128009, 128001,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  'labels': tensor([-100, -100, -100,  ..., -100, -100, -100]),\n",
       "  'position_ids': tensor([65310, 65311, 65312, 65313, 65314, 65315, 65316, 65317, 65318, 65319,\n",
       "          65320, 65321, 65322, 65323, 65324, 65325, 65326, 65327, 65328, 65329,\n",
       "          65330, 65331, 65332, 65333, 65334, 65335, 65336, 65337, 65338, 65339,\n",
       "          65340, 65341, 65342, 65343, 65344, 65345, 65346, 65347, 65348, 65349,\n",
       "          65350, 65351, 65352, 65353, 65354, 65355, 65356, 65357, 65358, 65359,\n",
       "          65360, 65361, 65362, 65363, 65364, 65365, 65366, 65367, 65368, 65369,\n",
       "          65370, 65371, 65372, 65373, 65374, 65375, 65376, 65377, 65378, 65379,\n",
       "          65380, 65381, 65382, 65383, 65384, 65385, 65386, 65387, 65388, 65389,\n",
       "          65390, 65391, 65392, 65393, 65394, 65395, 65396, 65397, 65398, 65399,\n",
       "          65400, 65401, 65402, 65403, 65404, 65405, 65406, 65407, 65408, 65409,\n",
       "          65410, 65411, 65412, 65413, 65414, 65415, 65416, 65417, 65418, 65419,\n",
       "          65420, 65421, 65422, 65423, 65424, 65425, 65426, 65427, 65428, 65429,\n",
       "          65430, 65431, 65432, 65433, 65434, 65435, 65436, 65437, 65438, 65439,\n",
       "          65440, 65441, 65442, 65443, 65444, 65445, 65446, 65447, 65448, 65449,\n",
       "          65450, 65451, 65452, 65453, 65454, 65455, 65456, 65457, 65458, 65459,\n",
       "          65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0])}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datassets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 1045, 2069, 3093, 4117, 5141, 6165, 7189, 8213, 9237, 10261, 11285, 12309, 13333, 14357, 15381, 16405, 16430]\n",
      "tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([   21,  4099,  8177, 12255, 16333, 20411, 24489, 28567, 32645, 36723,\n",
      "        40801, 44879, 48957, 53035, 57113, 61191, 65269, 65305])\n",
      "tensor([   20,     0,  5122,     0,     0,     0,     0, 27548,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0, 65304])\n"
     ]
    }
   ],
   "source": [
    "all_spe_pos = all_datassets[0]['all_spe_pos']\n",
    "input_ids = all_datassets[0]['input_ids']\n",
    "position_ids = all_datassets[0]['position_ids']\n",
    "print(all_spe_pos)\n",
    "shift_spe_pos = np.array(all_spe_pos) - 1\n",
    "selected_ids = input_ids[all_spe_pos]\n",
    "selected_position_ids = position_ids[all_spe_pos]\n",
    "selected_shift_position_ids = position_ids[shift_spe_pos]\n",
    "print(selected_ids)\n",
    "print(selected_position_ids)\n",
    "print(selected_shift_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97mpkl file saved successfully!\u001b[0m\n",
      "\u001b[32mSave file to /data/zecheng/data/processed_project/mix_chunks_v3/combine/train.pkl | len: 25576 |  size: 18.56 GB\u001b[0m\n",
      "\u001b[30mpkl file saved successfully!\u001b[0m\n",
      "\u001b[32mSave file to /data/zecheng/data/processed_project/mix_chunks_v3/combine/valid.pkl | len: 200 |  size: 148.57 MB\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_sample = training_samples[200:]\n",
    "valid_sample = training_samples[:200]\n",
    "\n",
    "auto_save_data(train_sample, \"/data/zecheng/data/processed_project/mix_chunks_v3/combine/train.pkl\", show_meta_data=True)\n",
    "auto_save_data(valid_sample, \"/data/zecheng/data/processed_project/mix_chunks_v3/combine/valid.pkl\", show_meta_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.DataFrame(training_samples)\n",
    "\n",
    "with tqdm(total=len(df.columns), desc=\"Converting tensors to numpy\") as pbar_outer:\n",
    "    for column in df.columns:\n",
    "        if isinstance(df[column][0], torch.Tensor):\n",
    "            with tqdm(total=len(df[column]), desc=f\"Processing {column}\") as pbar_inner:\n",
    "\n",
    "                def convert_tensor(x):\n",
    "                    result = x.numpy().tolist()\n",
    "                    pbar_inner.update(1)\n",
    "                    return result\n",
    "\n",
    "                df[column] = df[column].apply(convert_tensor)\n",
    "        pbar_outer.update(1)\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_data = dataset.from_dict(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn_train(feature):\n",
    "    lst = (np.array(feature['concatenated_labels'])!= -100).sum(-1).tolist()\n",
    "    return all(n > 15 for n in lst)\n",
    "\n",
    "def filter_fn_valid(feature):\n",
    "    lst = (np.array(feature['concatenated_labels'])!= -100).sum(-1).tolist()\n",
    "    return all(n > 0 for n in lst)\n",
    "\n",
    "def filter_fn_input_length(feature):\n",
    "    ipt_length = np.array(feature[\"concatenated_input_ids\"]).shape[-1] \n",
    "    return ipt_length == 17408\n",
    "\n",
    "\n",
    "# split the dataset into training and validation\n",
    "# hf_data = convert_jsonl_to_dict(training_samples, format=\"hf\")\n",
    "hf_data = dataset.from_dict(training_samples)\n",
    "train_test_data = hf_data.train_test_split(test_size=400)\n",
    "dataset_dict = DatasetDict({'train': train_test_data['train'], 'valid': train_test_data['test']})\n",
    "\n",
    "\n",
    "# filter_train_data = all_data['train'].filter(filter_fn_train, num_proc=32)\n",
    "# filter_valid_data = all_data['valid'].filter(filter_fn_valid, num_proc=32)\n",
    "\n",
    "# filter_data = DatasetDict({\"train\": filter_train_data, \"valid\": filter_valid_data})\n",
    "\n",
    "# print(\"before filtering\")\n",
    "# print(all_data)\n",
    "# print(\"after filtering\")\n",
    "# print(filter_data)\n",
    "\n",
    "# filter_data.save_to_disk(save_path, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_64_cache_v2\"\n",
    "all_data = load_from_disk(save_path)\n",
    "print(all_data)\n",
    "\n",
    "def filter_fn_input_length(feature):\n",
    "    ipt_length = np.array(feature[\"concatenated_input_ids\"]).shape[-1] \n",
    "    return ipt_length == 17408\n",
    "\n",
    "filter_data = all_data.filter(filter_fn_input_length, num_proc=48)\n",
    "\n",
    "print(filter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data.save_to_disk(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_64_cache_v4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
