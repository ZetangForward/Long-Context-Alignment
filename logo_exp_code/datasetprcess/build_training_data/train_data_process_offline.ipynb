{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[36mModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-07-02 17:31:04\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "from datasets import Dataset, load_dataset, load_from_disk, DatasetDict\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from contextlib import contextmanager, nullcontext\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Union, Literal, Dict\n",
    "from modelzipper.tutils import *\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from scipy.stats import qmc\n",
    "import numpy as np\n",
    "import itertools\n",
    "from functools import partial\n",
    "from accelerate import PartialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create POSE and Padding Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3, 4]), tensor([3, 4, 5, 6, 7]), tensor([ 7,  8,  9, 10, 11])]\n"
     ]
    }
   ],
   "source": [
    "def find_index(id_lst, prefix_id, suffix_id):\n",
    "    return id_lst.index(prefix_id), id_lst.index(suffix_id)\n",
    "\n",
    "\n",
    "def create_position_ids(N, L):\n",
    "    \"\"\"sampling N points from L (max_chunk_size space)\"\"\"\n",
    "    if N == L:\n",
    "        start_pos = 0\n",
    "    else:\n",
    "        start_pos = np.random.randint(0, L - N)\n",
    "    end_pos = start_pos + N\n",
    "    position_ids = torch.arange(start_pos, end_pos)\n",
    "    return position_ids\n",
    "\n",
    "\n",
    "def create_covering_position_ids(N, L):\n",
    "    \"\"\"Create sets of position IDs to cover all positions from 0 to L-1 with intervals of length N.\"\"\"\n",
    "    if N > L:\n",
    "        raise ValueError(\"N should not be greater than L\")\n",
    "    num_intervals = (L + N - 1) // N\n",
    "    position_ids_list = []\n",
    "    for i in range(num_intervals):\n",
    "        start_pos = i * (L - N) // (num_intervals - 1) if num_intervals > 1 else 0\n",
    "        end_pos = start_pos + N\n",
    "        if end_pos > L:\n",
    "            end_pos = L\n",
    "            start_pos = L - N if L > N else 0\n",
    "        position_ids = torch.arange(start_pos, end_pos)\n",
    "        position_ids_list.append(position_ids)\n",
    "    return position_ids_list\n",
    "\n",
    "def auto_padding(t: torch.Tensor, length: int, filling_value=-100, return_attention_mask=False):\n",
    "    if length < t.size(0):\n",
    "        if return_attention_mask: \n",
    "            return t[:length]\n",
    "        else: \n",
    "            return t[:length], torch.ones_like(t[:length])\n",
    "    padded_tensor = torch.full((length,), filling_value, dtype=t.dtype)\n",
    "    padded_tensor[:t.size(0)] = t\n",
    "    if return_attention_mask:\n",
    "        attention_mask = torch.zeros(length, dtype=torch.int)\n",
    "        attention_mask[:t.size(0)] = 1\n",
    "        return padded_tensor, attention_mask\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "N = 5\n",
    "L = 12\n",
    "\n",
    "position_ids_list = create_covering_position_ids(N, L)\n",
    "print(position_ids_list)\n",
    "# for idx, pos_ids in enumerate(position_ids_list):\n",
    "#     print(f\"Position IDs {idx+1}:\", pos_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19])\n",
      "torch.Size([19])\n",
      "torch.Size([19])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n"
     ]
    }
   ],
   "source": [
    "def combine_fn(lst, max_candidates=2, max_combination=16):\n",
    "    trimmed_lists = [random.sample(sublst, min(len(sublst), max_candidates)) if len(sublst) > max_candidates else sublst for sublst in lst]\n",
    "    all_combinations = itertools.product(*trimmed_lists)\n",
    "    concatenated_results = [torch.cat(combination) for combination in all_combinations]\n",
    "    concatenated_results = random.sample(concatenated_results, min(len(concatenated_results), max_combination))\n",
    "    return concatenated_results\n",
    "\n",
    "def create_system_suffix(tokenizer, system_suffix, special_token_id: int=13):\n",
    "    tok_suffix = tokenizer(system_suffix, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    padded_tok_suffix, padded_suffix_attention_mask = tok_suffix.input_ids[0], tok_suffix.attention_mask[0]\n",
    "    # add special token\n",
    "    padded_tok_suffix = torch.concatenate([padded_tok_suffix, torch.tensor([special_token_id])], dim=0)\n",
    "    padded_suffix_attention_mask = torch.concatenate([padded_suffix_attention_mask, torch.tensor([1])], dim=0)\n",
    "    system_position_ids = torch.arange(0, padded_tok_suffix.size(-1))\n",
    "    return padded_tok_suffix, padded_suffix_attention_mask, system_position_ids\n",
    "\n",
    "def create_chunked_reference(tokenizer: AutoTokenizer, all_refs: List[str], real_reference_size: int, max_embedding_size: int, system_prompt_size: int, qa_size: int, special_token_id: int=13):\n",
    "    real_max_chunk_size = real_reference_size // len(all_refs) - 1 # allocate one position for attention reallocation\n",
    "    tok_all_ref = [tokenizer(item, return_tensors=\"pt\", add_special_tokens=False).input_ids[0] for item in all_refs]\n",
    "    truncted_refer_tok_lst, statistic_data_size = [], []\n",
    "    for item in tok_all_ref:\n",
    "        statistic_data_size.append(item.size(-1))\n",
    "        if item.size(-1) > real_max_chunk_size: \n",
    "            item = item[: real_max_chunk_size]\n",
    "        truncted_refer_tok_lst.append(item)\n",
    "\n",
    "    fake_position_chunk_size = (max_embedding_size - qa_size - system_prompt_size) // len(tok_all_ref)  # with last special token index for each chunk\n",
    "    positional_chunks = torch.arange(system_prompt_size, max_embedding_size - qa_size, fake_position_chunk_size)\n",
    "    # Here, end_positional_chunks denotes special token ids\n",
    "    begin_positional_chunks, end_positional_chunks = positional_chunks[:-1], positional_chunks[1:] - 1  \n",
    "    all_chunk_pos_lst = []\n",
    "    \n",
    "    for i, item in enumerate(truncted_refer_tok_lst):\n",
    "        chunk_token_pos_lst = create_covering_position_ids(item.size(-1), fake_position_chunk_size-1)\n",
    "        chunk_token_pos_lst = [item + begin_positional_chunks[i] for item in chunk_token_pos_lst]\n",
    "        all_chunk_pos_lst.append(chunk_token_pos_lst)\n",
    "\n",
    "    padded_chunk_pos_lst = [[auto_padding(sub_item, real_max_chunk_size, filling_value=0, return_attention_mask=False) for sub_item in item] for item in all_chunk_pos_lst]\n",
    "    padded_refer_tok_lst = [auto_padding(item, real_max_chunk_size, filling_value=0, return_attention_mask=True) for item in truncted_refer_tok_lst]\n",
    "    padded_refer_tok_ids = [item[0] for item in padded_refer_tok_lst]\n",
    "    padded_refer_attention_mask = [item[1] for item in padded_refer_tok_lst]\n",
    "\n",
    "    candicated_padded_position_ids = []\n",
    "    padded_ref_input_ids_lst, padded_ref_attention_mask_lst = [], []\n",
    "    \n",
    "    for chunk_pos_ids, chunk_spe_pos_lst in zip(end_positional_chunks, padded_chunk_pos_lst):\n",
    "        tmp_chunk_pos_ids = []\n",
    "        for tmp in chunk_spe_pos_lst:\n",
    "            tmp = torch.concatenate([tmp, torch.tensor([chunk_pos_ids])], dim=0)\n",
    "            tmp_chunk_pos_ids.append(tmp)  # [[0,1,...,C1], [C2,C2+1,...,C3], ...]\n",
    "        candicated_padded_position_ids.append(tmp_chunk_pos_ids)\n",
    "    candicated_padded_position_ids = combine_fn(candicated_padded_position_ids, max_combination=32)\n",
    "\n",
    "    for padded_chunk_tok_ref_input_ids, padded_chunk_tok_ref_attention_mask in zip(padded_refer_tok_ids, padded_refer_attention_mask):\n",
    "        padded_chunk_tok_ref_input_ids = torch.concatenate([padded_chunk_tok_ref_input_ids, torch.tensor([special_token_id])], dim=0)\n",
    "        padded_chunk_tok_ref_attention_mask = torch.concatenate([padded_chunk_tok_ref_attention_mask, torch.tensor([1])], dim=0)\n",
    "        padded_ref_input_ids_lst.append(padded_chunk_tok_ref_input_ids)\n",
    "        padded_ref_attention_mask_lst.append(padded_chunk_tok_ref_attention_mask)\n",
    "    \n",
    "    padded_ref_input_ids = torch.concatenate(padded_ref_input_ids_lst, dim=0)\n",
    "    padded_ref_attention_mask = torch.concatenate(padded_ref_attention_mask_lst, dim=0)\n",
    "    all_spe_pos = torch.arange(real_max_chunk_size, real_reference_size, real_max_chunk_size + 1)\n",
    "\n",
    "    return candicated_padded_position_ids, padded_ref_input_ids, padded_ref_attention_mask, all_spe_pos\n",
    "\n",
    "\n",
    "def create_qa(QUESTION_TEMPLATE, ANSWER_TEMPLATE, combined_question, combined_answer, prefix_a: str, suffix_a: str, last_position: int, qa_size: int, special_token_id: int):\n",
    "    \"\"\"\n",
    "    last_position 是reference position ids最大的数值，下面的代码要加一个 last_position + 1的shift\n",
    "    qa_size 规定了最大的qa 的长度，所以总长度需要手动卡一下\n",
    "    \"\"\"\n",
    "    # Create Question\n",
    "    question = QUESTION_TEMPLATE.format(question=combined_question)\n",
    "    tok_question = tokenizer(question, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    padded_tok_question, padded_question_attention_mask = auto_padding(tok_question, tok_question.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    padded_tok_question = torch.concatenate([padded_tok_question, torch.tensor([special_token_id])], dim=0)\n",
    "    padded_question_attention_mask = torch.concatenate([padded_question_attention_mask, torch.tensor([1])], dim=0)\n",
    "    question_position_input_ids = create_position_ids(tok_question.size(-1), tok_question.size(-1)) + last_position + 1\n",
    "    last_pos = question_position_input_ids.max() + 1\n",
    "    question_position_input_ids = torch.concatenate([question_position_input_ids, torch.tensor([last_pos])], dim=0)\n",
    "    spe_tok_pos = question_position_input_ids.size(-1) - 1\n",
    "\n",
    "    # Create Chosen / Rejected Answers / and their labels\n",
    "    chosen_answer = ANSWER_TEMPLATE.format(answer=combined_answer)\n",
    "    prefix_rejected_answer = ANSWER_TEMPLATE.format(answer=prefix_a)\n",
    "    suffix_rejected_answer = ANSWER_TEMPLATE.format(answer=suffix_a)\n",
    "    tok_chosen_answer = tokenizer(chosen_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    tok_prefix_rejected_answer = tokenizer(prefix_rejected_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    tok_suffix_rejected_answer = tokenizer(suffix_rejected_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    system_reference_question_size = last_position + 1 + question_position_input_ids.size(-1)\n",
    "\n",
    "    padded_tok_chosen_answer, padded_chosen_answer_attention_mask = auto_padding(tok_chosen_answer, qa_size-padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_chosen_answer_labels = auto_padding(tok_chosen_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    chosen_answer_position_ids = create_position_ids(tok_chosen_answer.size(-1), tok_chosen_answer.size(-1)) + system_reference_question_size\n",
    "    chosen_answer_position_ids = auto_padding(chosen_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "\n",
    "    padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_prefix_rejected_answer_labels = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    prefix_rejected_answer_position_ids = create_position_ids(tok_prefix_rejected_answer.size(-1), tok_prefix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    prefix_rejected_answer_position_ids = auto_padding(prefix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "    \n",
    "    padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_suffix_rejected_answer_labels = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    suffix_rejected_answer_position_ids = create_position_ids(tok_suffix_rejected_answer.size(-1), tok_suffix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    suffix_rejected_answer_position_ids = auto_padding(suffix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    " \n",
    "    return padded_tok_question, padded_question_attention_mask, question_position_input_ids, \\\n",
    "        padded_tok_chosen_answer, padded_chosen_answer_attention_mask, tok_chosen_answer_labels, chosen_answer_position_ids, \\\n",
    "        padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, prefix_rejected_answer_position_ids, \\\n",
    "        padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, suffix_rejected_answer_position_ids, spe_tok_pos\n",
    "    \n",
    "\n",
    "\"\"\"block testing create_chunked_reference\"\"\" \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/data/zecheng/hf_models/Meta-Llama-3-8B-Instruct\")\n",
    "all_refs = [\"hello, world\", \"Any, iowpq\", \"reason medsa\"]\n",
    "real_reference_size = 28\n",
    "system_prompt_size, qa_size = 2, 4\n",
    "max_embedding_size = 3400\n",
    "candicated_padded_concat_position_ids, padded_ref_input_ids, padded_ref_attention_mask, all_spe_pos = create_chunked_reference(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size)\n",
    "\n",
    "\"\"\"Test Create QA Function\"\"\"\n",
    "QUESTION_TEMPLATE = \"<|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the following question according to the references: {question}<|eot_id|>\"\n",
    "ANSWER_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\\n\\nThe answer is: {answer}<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "padded_tok_question, padded_question_attention_mask, \\\n",
    "question_position_input_ids, padded_tok_chosen_answer, \\\n",
    "padded_chosen_answer_attention_mask, tok_chosen_answer_labels, \\\n",
    "chosen_answer_position_ids, padded_tok_prefix_rejected_answer, \\\n",
    "padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, \\\n",
    "prefix_rejected_answer_position_ids, padded_tok_suffix_rejected_answer, \\\n",
    "padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, \\\n",
    "suffix_rejected_answer_position_ids, spe_tok_pos = create_qa(QUESTION_TEMPLATE, ANSWER_TEMPLATE, \"who are you\", \"jack\", \"prefix_a\", \"suffix_a\", last_position=1024, qa_size=100, special_token_id=13)\n",
    "\n",
    "print(padded_tok_question.shape)\n",
    "print(padded_question_attention_mask.shape)\n",
    "print(question_position_input_ids.shape)\n",
    "print(padded_tok_chosen_answer.shape)\n",
    "print(chosen_answer_position_ids.shape)\n",
    "print(prefix_rejected_answer_position_ids.shape)\n",
    "print(padded_tok_suffix_rejected_answer.shape)\n",
    "print(padded_suffix_rejected_answer_attention_mask.shape)\n",
    "print(suffix_rejected_answer_position_ids.shape)\n",
    "print(tok_suffix_rejected_answer_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Current Avg Seq Length: 31.95:   0%|          | 4/1032 [00:08<36:16,  2.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# prefix_id, suffix_id = find_index(item[\"all_ref_ids\"], item[\"prefix_id\"], item[\"suffix_id\"])\u001b[39;00m\n\u001b[1;32m     99\u001b[0m prefix_id, suffix_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m all_datasets, ref_length \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_covering_position_ipt_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_ref_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_embedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m65536\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_reference_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspe_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m avg_real_seq_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ref_length \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m    102\u001b[0m training_samples\u001b[38;5;241m.\u001b[39mextend(all_datasets)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mcreate_covering_position_ipt_data\u001b[0;34m(tokenizer, all_refs, combined_question, combined_answer, prefix_a, suffix_a, qa_size, max_embedding_size, real_reference_size, special_token_id, prefix_id, suffix_id)\u001b[0m\n\u001b[1;32m     11\u001b[0m all_spe_pos \u001b[38;5;241m=\u001b[39m [system_prompt_size\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# create chunk reference (input_ids, attention_mask and positional ids)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m candicated_padded_position_ids_lst, padded_ref_input_ids, padded_ref_attention_mask, ref_spe_pos \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_chunked_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_reference_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_embedding_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecial_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m ref_spe_pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m system_prompt_size\n\u001b[1;32m     16\u001b[0m all_spe_pos\u001b[38;5;241m.\u001b[39mextend(ref_spe_pos\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mcreate_chunked_reference\u001b[0;34m(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, special_token_id)\u001b[0m\n\u001b[1;32m     50\u001b[0m         tmp_chunk_pos_ids\u001b[38;5;241m.\u001b[39mappend(tmp)  \u001b[38;5;66;03m# [[0,1,...,C1], [C2,C2+1,...,C3], ...]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     candicated_padded_position_ids\u001b[38;5;241m.\u001b[39mappend(tmp_chunk_pos_ids)\n\u001b[0;32m---> 52\u001b[0m candicated_padded_position_ids \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandicated_padded_position_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_combination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m padded_chunk_tok_ref_input_ids, padded_chunk_tok_ref_attention_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(padded_refer_tok_ids, padded_refer_attention_mask):\n\u001b[1;32m     55\u001b[0m     padded_chunk_tok_ref_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcatenate([padded_chunk_tok_ref_input_ids, torch\u001b[38;5;241m.\u001b[39mtensor([special_token_id])], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mcombine_fn\u001b[0;34m(lst, max_candidates, max_combination)\u001b[0m\n\u001b[1;32m      2\u001b[0m trimmed_lists \u001b[38;5;241m=\u001b[39m [random\u001b[38;5;241m.\u001b[39msample(sublst, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sublst), max_candidates)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sublst) \u001b[38;5;241m>\u001b[39m max_candidates \u001b[38;5;28;01melse\u001b[39;00m sublst \u001b[38;5;28;01mfor\u001b[39;00m sublst \u001b[38;5;129;01min\u001b[39;00m lst]\n\u001b[1;32m      3\u001b[0m all_combinations \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mtrimmed_lists)\n\u001b[0;32m----> 4\u001b[0m concatenated_results \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat(combination) \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m all_combinations]\n\u001b[1;32m      5\u001b[0m concatenated_results \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(concatenated_results, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(concatenated_results), max_combination))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenated_results\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m trimmed_lists \u001b[38;5;241m=\u001b[39m [random\u001b[38;5;241m.\u001b[39msample(sublst, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sublst), max_candidates)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sublst) \u001b[38;5;241m>\u001b[39m max_candidates \u001b[38;5;28;01melse\u001b[39;00m sublst \u001b[38;5;28;01mfor\u001b[39;00m sublst \u001b[38;5;129;01min\u001b[39;00m lst]\n\u001b[1;32m      3\u001b[0m all_combinations \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;241m*\u001b[39mtrimmed_lists)\n\u001b[0;32m----> 4\u001b[0m concatenated_results \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m all_combinations]\n\u001b[1;32m      5\u001b[0m concatenated_results \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(concatenated_results, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(concatenated_results), max_combination))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenated_results\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_covering_position_ipt_data(tokenizer, all_refs: List[str], combined_question: str, combined_answer: str, prefix_a: str, suffix_a: str, qa_size: int, max_embedding_size: int, real_reference_size: int, special_token_id: int = None, prefix_id: int = None, suffix_id: int = None):\n",
    "    statistic_data_size = []\n",
    "\n",
    "    SYSTEM_SUFFIX = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nBelow is some references. Please read it carefully and answer the following question.<|eot_id|>\"\n",
    "    QUESTION_TEMPLATE = \"<|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the following question according to the references: {question}<|eot_id|>\"\n",
    "    ANSWER_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\\n\\nThe answer is: {answer}<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "    # Create System Suffix\n",
    "    padded_tok_input_ids_system_suffix, padded_attention_mask_system_suffix, padded_position_ids_system_suffix = create_system_suffix(tokenizer, SYSTEM_SUFFIX, special_token_id)\n",
    "    system_prompt_size = padded_attention_mask_system_suffix.size(-1)\n",
    "    all_spe_pos = [system_prompt_size-1]\n",
    "    # create chunk reference (input_ids, attention_mask and positional ids)\n",
    "    candicated_padded_position_ids_lst, padded_ref_input_ids, padded_ref_attention_mask, ref_spe_pos = create_chunked_reference(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, special_token_id)\n",
    "\n",
    "    ref_spe_pos += system_prompt_size\n",
    "    all_spe_pos.extend(ref_spe_pos.tolist())\n",
    "\n",
    "    # combine and wrap each position_id, input_ids and attention_mask\n",
    "    last_position = max_embedding_size - qa_size  # size for real reference and system prompt\n",
    "\n",
    "    # Create Question, all Answers\n",
    "    padded_tok_question, padded_question_attention_mask, \\\n",
    "    question_position_input_ids, padded_tok_chosen_answer, \\\n",
    "    padded_chosen_answer_attention_mask, tok_chosen_answer_labels, \\\n",
    "    chosen_answer_position_ids, padded_tok_prefix_rejected_answer, \\\n",
    "    padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, \\\n",
    "    prefix_rejected_answer_position_ids, padded_tok_suffix_rejected_answer, \\\n",
    "    padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, \\\n",
    "    suffix_rejected_answer_position_ids, spe_tok_pos = create_qa(\n",
    "        QUESTION_TEMPLATE, ANSWER_TEMPLATE, combined_question, combined_answer, prefix_a, suffix_a, last_position, qa_size, special_token_id=special_token_id\n",
    "    )\n",
    "    all_spe_pos.append(spe_tok_pos + system_prompt_size + padded_ref_input_ids.size(-1))\n",
    "    all_datasets = []  # different combination of positions \n",
    "\n",
    "    for i, ref_position_id in enumerate(candicated_padded_position_ids_lst):\n",
    "        concatenated_batch = {}\n",
    "        concatenated_batch[\"input_ids\"] = torch.concatenate([padded_tok_input_ids_system_suffix, padded_ref_input_ids, padded_tok_question], dim=0)\n",
    "        concatenated_batch[\"attention_mask\"] = torch.concatenate([padded_attention_mask_system_suffix, padded_ref_attention_mask, padded_question_attention_mask], dim=0)\n",
    "        concatenated_batch[\"position_ids\"] = torch.concatenate([padded_position_ids_system_suffix, ref_position_id, question_position_input_ids], dim=0)\n",
    "        referece_question_length = concatenated_batch[\"attention_mask\"].size(-1)\n",
    "        concatenated_batch[\"all_spe_pos\"] = all_spe_pos\n",
    "        referece_question_labels = torch.full((1, referece_question_length), -100)[0]\n",
    "        \n",
    "        # Create Labels for Each Part\n",
    "        concatenated_batch[\"chosen_answer\"] = {\n",
    "            \"input_ids\": padded_tok_chosen_answer, \n",
    "            \"attention_mask\": padded_chosen_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_chosen_answer_labels], dim=0),\n",
    "            \"position_ids\": chosen_answer_position_ids\n",
    "        }\n",
    "        concatenated_batch[\"prefix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_prefix_rejected_answer, \n",
    "            \"attention_mask\": padded_prefix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_prefix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": prefix_rejected_answer_position_ids,\n",
    "        }\n",
    "        concatenated_batch[\"suffix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_suffix_rejected_answer, \n",
    "            \"attention_mask\": padded_suffix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_suffix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": suffix_rejected_answer_position_ids,\n",
    "        }\n",
    "        if prefix_id is not None and suffix_id is not None:\n",
    "            concatenated_batch[\"chosen_ids\"] = (prefix_id, suffix_id)\n",
    "        all_datasets.append(concatenated_batch)\n",
    "        statistic_data_size.append(concatenated_batch[\"input_ids\"].size(-1))\n",
    "\n",
    "    return all_datasets, sum(statistic_data_size) / len(statistic_data_size)\n",
    "\n",
    "\n",
    "def map_fn(item, tokenizer, special_token_id, qa_size, max_embedding_size, real_reference_size, prefix_id, suffix_id):\n",
    "    # 暂时没有使用，只是为了大数据的快速构造而写的code\n",
    "    all_ref_text = item[\"all_ref_text\"]\n",
    "    combined_question, final_answer = item[\"combined_question\"], item[\"final_answer\"]\n",
    "    prefix_q, suffix_q = item[\"prefix_q\"], item[\"suffix_q\"]\n",
    "    prefix_a, suffix_a = item[\"prefix_a\"], item[\"suffix_a\"]\n",
    "    prefix_id, suffix_id = None, None\n",
    "    all_datasets, ref_length = create_covering_position_ipt_data(tokenizer, all_ref_text, combined_question, final_answer, prefix_a, suffix_a, qa_size=qa_size, max_embedding_size=max_embedding_size, real_reference_size=real_reference_size, special_token_id=special_token_id, prefix_id=prefix_id, suffix_id=suffix_id)\n",
    "    result_dict = {f'column_{i}': all_datasets[i] for i in range(len(all_datasets))}\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "dataset = datasets.load_from_disk(\"/data/zecheng/data/processed_project/16chunk/1024_chunk_size/hf_data_step3\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/data/zecheng/hf_models/Meta-Llama-3-8B-Instruct\")\n",
    "training_samples = []\n",
    "avg_real_seq_length = 0\n",
    "spe_token_id = tokenizer(\"<|reserved_special_token_0|>\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "# processed_data = dataset.map(map_fn, fn_kwargs={\"tokenizer\": tokenizer, \"special_token_id\": spe_token_id, \"qa_size\": 256, \"max_embedding_size\": 65536, \"real_reference_size\": 8192, \"prefix_id\": None, \"suffix_id\": None}, num_proc=16)\n",
    "\n",
    "with tqdm(total=len(dataset), desc=f\"Initial Avg Length: {avg_real_seq_length}\") as pbar:\n",
    "    for item in dataset:\n",
    "        all_ref_text = item[\"all_ref_text\"]\n",
    "        combined_question, final_answer = item[\"combined_question\"], item[\"final_answer\"]\n",
    "        prefix_q, suffix_q = item[\"prefix_q\"], item[\"suffix_q\"]\n",
    "        prefix_a, suffix_a = item[\"prefix_a\"], item[\"suffix_a\"]\n",
    "        # prefix_id, suffix_id = find_index(item[\"all_ref_ids\"], item[\"prefix_id\"], item[\"suffix_id\"])\n",
    "        prefix_id, suffix_id = None, None\n",
    "        all_datasets, ref_length = create_covering_position_ipt_data(tokenizer, all_ref_text, combined_question, final_answer, prefix_a, suffix_a, qa_size=256, max_embedding_size=65536, real_reference_size=16384, special_token_id=spe_token_id, prefix_id=prefix_id, suffix_id=suffix_id)\n",
    "        avg_real_seq_length += ref_length / len(dataset)\n",
    "        training_samples.extend(all_datasets)\n",
    "        pbar.set_description(f\"Current Avg Seq Length: {avg_real_seq_length:.2f}\")\n",
    "        pbar.update(1)\n",
    "print(len(training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spe_pos = training_samples[0]['all_spe_pos']\n",
    "input_ids = training_samples[0]['input_ids']\n",
    "position_ids = training_samples[0]['position_ids']\n",
    "print(all_spe_pos)\n",
    "shift_spe_pos = np.array(all_spe_pos) - 1\n",
    "selected_ids = input_ids[all_spe_pos]\n",
    "selected_position_ids = position_ids[all_spe_pos]\n",
    "selected_shift_position_ids = position_ids[shift_spe_pos]\n",
    "print(selected_ids)\n",
    "print(selected_position_ids)\n",
    "print(selected_shift_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = training_samples[500:]\n",
    "valid_sample = training_samples[:500]\n",
    "\n",
    "auto_save_data(train_sample, \"/data/zecheng/data/processed_project/1024_chunk_size/step4_jsonl_data_less/train.pkl\", show_meta_data=True)\n",
    "auto_save_data(valid_sample, \"/data/zecheng/data/processed_project/1024_chunk_size/step4_jsonl_data_less/valid.pkl\", show_meta_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.DataFrame(training_samples)\n",
    "\n",
    "with tqdm(total=len(df.columns), desc=\"Converting tensors to numpy\") as pbar_outer:\n",
    "    for column in df.columns:\n",
    "        if isinstance(df[column][0], torch.Tensor):\n",
    "            with tqdm(total=len(df[column]), desc=f\"Processing {column}\") as pbar_inner:\n",
    "\n",
    "                def convert_tensor(x):\n",
    "                    result = x.numpy().tolist()\n",
    "                    pbar_inner.update(1)\n",
    "                    return result\n",
    "\n",
    "                df[column] = df[column].apply(convert_tensor)\n",
    "        pbar_outer.update(1)\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_data = dataset.from_dict(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn_train(feature):\n",
    "    lst = (np.array(feature['concatenated_labels'])!= -100).sum(-1).tolist()\n",
    "    return all(n > 15 for n in lst)\n",
    "\n",
    "def filter_fn_valid(feature):\n",
    "    lst = (np.array(feature['concatenated_labels'])!= -100).sum(-1).tolist()\n",
    "    return all(n > 0 for n in lst)\n",
    "\n",
    "def filter_fn_input_length(feature):\n",
    "    ipt_length = np.array(feature[\"concatenated_input_ids\"]).shape[-1] \n",
    "    return ipt_length == 17408\n",
    "\n",
    "\n",
    "# split the dataset into training and validation\n",
    "# hf_data = convert_jsonl_to_dict(training_samples, format=\"hf\")\n",
    "hf_data = dataset.from_dict(training_samples)\n",
    "train_test_data = hf_data.train_test_split(test_size=400)\n",
    "dataset_dict = DatasetDict({'train': train_test_data['train'], 'valid': train_test_data['test']})\n",
    "\n",
    "\n",
    "# filter_train_data = all_data['train'].filter(filter_fn_train, num_proc=32)\n",
    "# filter_valid_data = all_data['valid'].filter(filter_fn_valid, num_proc=32)\n",
    "\n",
    "# filter_data = DatasetDict({\"train\": filter_train_data, \"valid\": filter_valid_data})\n",
    "\n",
    "# print(\"before filtering\")\n",
    "# print(all_data)\n",
    "# print(\"after filtering\")\n",
    "# print(filter_data)\n",
    "\n",
    "# filter_data.save_to_disk(save_path, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_64_cache_v2\"\n",
    "all_data = load_from_disk(save_path)\n",
    "print(all_data)\n",
    "\n",
    "def filter_fn_input_length(feature):\n",
    "    ipt_length = np.array(feature[\"concatenated_input_ids\"]).shape[-1] \n",
    "    return ipt_length == 17408\n",
    "\n",
    "filter_data = all_data.filter(filter_fn_input_length, num_proc=48)\n",
    "\n",
    "print(filter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data.save_to_disk(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_64_cache_v4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
