{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[36mModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.7 <<< | AOE Time🕒 2024-06-28 00:40:49\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "from pprint import pprint\n",
    "import re\n",
    "import datasets\n",
    "import numpy as np\n",
    "from modelzipper.tutils import *\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fanoutQA_data = auto_read_data(\"/data/zecheng/Retrieval_Head/fanout-final-dev-fragments.json\")\n",
    "print(len(fanoutQA_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_evidence(raw_reference):\n",
    "    pattern = re.compile(r'<content>(.*?)</content>', re.DOTALL)\n",
    "    match = pattern.search(raw_reference)\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "def extract_all_evidence_iter(raw_reference):\n",
    "    pattern = re.compile(r'<content>(.*?)</content>', re.DOTALL)\n",
    "    matches = pattern.finditer(raw_reference)\n",
    "    results = [match.group(1) for match in matches]\n",
    "    return results\n",
    "\n",
    "\n",
    "def return_multi_hop_evidence_groups(root):\n",
    "    top_qa = {\"answer\": root['answer'], \"categories\": root['categories'], \"question\": root['question']}\n",
    "    decomposed_qa = find_evidence_groups(root, max_depth=2)\n",
    "    # second_hop = find_evidence_groups(root, max_depth=2)\n",
    "    # third_hop = find_evidence_groups(root, max_depth=3)\n",
    "    return {\"top_qa\": top_qa, \"two_hop\": decomposed_qa}\n",
    "    \n",
    "\n",
    "def find_evidence_groups(root, max_depth=2):\n",
    "    result = []\n",
    "\n",
    "    def dfs(node, depth):\n",
    "        if depth == max_depth:\n",
    "            candicate_evidence = find_evidence(node)\n",
    "            evidence = [extract_best_evidence(item) for item in candicate_evidence]\n",
    "            all_evidences = [extract_all_evidence_iter(item) for item in candicate_evidence]\n",
    "            return [{\n",
    "                \"question\": node[\"question\"], \n",
    "                \"answer\": node[\"answer\"], \n",
    "                \"evidence\": evidence[0], \n",
    "                \"depth\": depth,\n",
    "                \"all_evidences\": all_evidences,\n",
    "            }]\n",
    "        \n",
    "        else:\n",
    "            results = []\n",
    "            for child in node.get(\"decomposition\", []):\n",
    "                results.extend(dfs(child, depth + 1))\n",
    "            return results\n",
    "\n",
    "    def find_evidence(node):\n",
    "        if node.get(\"sorted_fragments\"):\n",
    "            return [node[\"sorted_fragments\"]]\n",
    "        evidence = []\n",
    "        for child in node.get(\"decomposition\", []):\n",
    "            evidence.extend(find_evidence(child))\n",
    "        return evidence\n",
    "\n",
    "    result.extend(dfs(root, 1))\n",
    "    return result\n",
    "\n",
    "testing_data = []\n",
    "with tqdm(total=len(fanoutQA_data)) as pbar:\n",
    "    for i, sample in enumerate(fanoutQA_data):\n",
    "        testing_data.append(return_multi_hop_evidence_groups(sample))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"processing finish, total cases {len(testing_data)}\")\n",
    "print(testing_data[0]['two_hop'][0]['all_evidences'][0][0])\n",
    "\n",
    "### ============ testing case ============ ###\n",
    "# CASE_ID = 3\n",
    "# res = return_multi_hop_evidence_groups(fanoutQA_data[CASE_ID])\n",
    "# print(len(res))\n",
    "# pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer(sample):\n",
    "    answer_str = \"\"\n",
    "    if isinstance(sample, list): \n",
    "        answer_str = \", \".join([str(i) for i in sample])\n",
    "    if isinstance(sample, dict):\n",
    "        for k, v in sample.items(): \n",
    "            answer_str += f\"{k}: {v}, \"\n",
    "        answer_str = answer_str[:-2]  # remove last \",\"\n",
    "    return answer_str\n",
    "\n",
    "closed_book_testing_sets, open_book_testing_sets = [], []\n",
    "\n",
    "for sample in testing_data:\n",
    "    answer = create_answer(sample[\"top_qa\"][\"answer\"])\n",
    "    if len(answer) == 0: continue\n",
    "    closed_book_testing_sets.append({\n",
    "        \"question\": sample[\"top_qa\"][\"question\"],\n",
    "        \"answer\": answer,\n",
    "    })\n",
    "    all_evidence = [item[\"all_evidences\"] for item in sample[\"two_hop\"]]\n",
    "    open_book_testing_sets.append({\n",
    "        \"question\": sample[\"top_qa\"][\"question\"],\n",
    "        \"answer\": answer,\n",
    "        \"all_evidence\": all_evidence,\n",
    "    })\n",
    "\n",
    "auto_save_data(closed_book_testing_sets, \"/data/zecheng/Retrieval_Head/quick_eval/fanoutqa_data/closed_book_testing_sets.jsonl\")\n",
    "auto_save_data(open_book_testing_sets, \"/data/zecheng/Retrieval_Head/quick_eval/fanoutqa_data/open_book_testing_sets.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open_book_testing_sets[0].keys())\n",
    "print(open_book_testing_sets[0]['question'])\n",
    "print(open_book_testing_sets[0]['answer'])\n",
    "open_book_testing_sets[0]['all_evidence'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single data\n",
    "def create_single_data_single_hop(sample):\n",
    "    question, answer = sample['first_hop']['question'], sample['first_hop']['answer']\n",
    "    answer_str = \"\"\n",
    "    if isinstance(answer, list): \n",
    "        answer_str = \", \".join([str(i) for i in answer])\n",
    "    if isinstance(answer, dict):\n",
    "        for k, v in answer.items(): \n",
    "            answer_str += f\"{k}: {v}, \"\n",
    "        answer_str = answer_str[:-2]  # remove last \",\"\n",
    "\n",
    "    all_evidences = []\n",
    "    \n",
    "    for sec_hop in sample['second_hop']:\n",
    "        hop_evidence = []\n",
    "        for evi in sec_hop['all_evidences']:\n",
    "            hop_evidence.extend(evi)\n",
    "        hop_evidence_str = \" [DOC] \".join(hop_evidence)\n",
    "        all_evidences.append(hop_evidence_str)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_str,\n",
    "        \"evidence\": all_evidences\n",
    "    }\n",
    "\n",
    "all_testing_sample = []\n",
    "for sample in testing_data:\n",
    "    all_testing_sample.append(create_single_data_single_hop(sample))\n",
    "    \n",
    "auto_save_data(all_testing_sample, \"/data/zecheng/sunzc/LongBench-main/data/fanoutqa/fanoutqa_1hop.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process simpo training data offline\n",
    "\n",
    "#### Preferred Data Format\n",
    "\n",
    "wrap_batch[\"concatenated_input_ids\"] = torch.tensor(batch[0][\"concatenated_input_ids\"])\n",
    "wrap_batch[\"concatenated_attention_mask\"] = torch.tensor(batch[0][\"concatenated_attention_mask\"])\n",
    "wrap_batch[\"concatenated_labels\"] = torch.tensor(batch[0][\"concatenated_labels\"])\n",
    "wrap_batch[\"position_ids\"] = torch.tensor(batch[0][\"position_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 2, 3, 4]), tensor([3, 4, 5, 6, 7]), tensor([ 7,  8,  9, 10, 11])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_position_ids(N, L):\n",
    "    \"\"\"sampling N points from L (max_chunk_size space)\"\"\"\n",
    "    if N == L:\n",
    "        start_pos = 0\n",
    "    else:\n",
    "        start_pos = np.random.randint(0, L - N)\n",
    "    end_pos = start_pos + N\n",
    "    position_ids = torch.arange(start_pos, end_pos)\n",
    "    return position_ids\n",
    "\n",
    "def create_covering_position_ids(N, L):\n",
    "    \"\"\"Create sets of position IDs to cover all positions from 0 to L-1 with intervals of length N.\"\"\"\n",
    "    if N > L:\n",
    "        raise ValueError(\"N should not be greater than L\")\n",
    "    \n",
    "    num_intervals = (L + N - 1) // N\n",
    "\n",
    "    position_ids_list = []\n",
    "    for i in range(num_intervals):\n",
    "        start_pos = i * (L - N) // (num_intervals - 1) if num_intervals > 1 else 0\n",
    "        end_pos = start_pos + N\n",
    "        if end_pos > L:\n",
    "            end_pos = L\n",
    "            start_pos = L - N if L > N else 0\n",
    "        position_ids = torch.arange(start_pos, end_pos)\n",
    "        position_ids_list.append(position_ids)\n",
    "\n",
    "    return position_ids_list\n",
    "\n",
    "def auto_padding(t: torch.Tensor, length: int, filling_value=-100, return_attention_mask=False):\n",
    "    if length < t.size(0):\n",
    "        if return_attention_mask: \n",
    "            return t[:length]\n",
    "        else: \n",
    "            return t[:length], torch.ones_like(t[:length])\n",
    "    padded_tensor = torch.full((length,), filling_value, dtype=t.dtype)\n",
    "    padded_tensor[:t.size(0)] = t\n",
    "    if return_attention_mask:\n",
    "        attention_mask = torch.zeros(length, dtype=torch.int)\n",
    "        attention_mask[:t.size(0)] = 1\n",
    "        return padded_tensor, attention_mask\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "N = 5\n",
    "L = 12\n",
    "\n",
    "position_ids_list = create_covering_position_ids(N, L)\n",
    "print(position_ids_list)\n",
    "# for idx, pos_ids in enumerate(position_ids_list):\n",
    "#     print(f\"Position IDs {idx+1}:\", pos_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_position_ipt_data(tokenizer: AutoTokenizer, all_refs: List[str], combined_question: str, combined_answer: str, prefix_a: str, suffix_a: str, qa_size: int, max_embedding_size: int, real_reference_size: int):\n",
    "\n",
    "    SYSTEM_SUFFIX = \"Below is some references. Please read it carefully and answer the following question: \"\n",
    "    QUESTION_TEMPLATE = \"Please answer the following question according to the references: {question}\\n\"\n",
    "    ANSWER_TEMPLATE = \"The answer is: {answer}\\n\"\n",
    "\n",
    "    # Create System Suffix\n",
    "    tok_suffix = tokenizer(SYSTEM_SUFFIX, return_tensors=\"pt\")\n",
    "    padded_tok_suffix, padded_suffix_attention_mask = tok_suffix.input_ids[0][:-1], tok_suffix.attention_mask[0][:-1]\n",
    "    system_position_ids = torch.arange(0, padded_tok_suffix.size(-1))\n",
    "    system_prompt_size = system_position_ids.size(-1)\n",
    "\n",
    "    # Create Chunked References\n",
    "    statistic_data_size = []\n",
    "    real_max_chunk_size = real_reference_size // len(all_refs)\n",
    "    tok_all_ref = [tokenizer(item, return_tensors=\"pt\", add_special_tokens=False).input_ids[0] for item in all_refs]\n",
    "    truncted_refer_tok_lst = []\n",
    "    for item in tok_all_ref:\n",
    "        statistic_data_size.append(item.size(-1))\n",
    "        if item.size(-1) > real_max_chunk_size: \n",
    "            item = item[: real_max_chunk_size]\n",
    "        truncted_refer_tok_lst.append(item)\n",
    "\n",
    "    position_input_ids = []\n",
    "    fake_position_chunk_size = (max_embedding_size - qa_size - system_prompt_size) // len(truncted_refer_tok_lst)\n",
    "    positional_chunks = torch.arange(system_prompt_size, max_embedding_size - qa_size, fake_position_chunk_size)\n",
    "    for i, item in enumerate(truncted_refer_tok_lst):\n",
    "        chunk_ids = create_position_ids(item.size(-1), real_max_chunk_size)\n",
    "        chunk_ids += positional_chunks[i]\n",
    "        position_input_ids.append(chunk_ids)\n",
    "\n",
    "    padded_input_attention_ids = [\n",
    "        auto_padding(item, real_max_chunk_size, filling_value=0, return_attention_mask=True) \n",
    "        for item in position_input_ids\n",
    "    ]\n",
    "    padded_reference_ids = [item[0] for item in padded_input_attention_ids]\n",
    "    padded_reference_ids = torch.concatenate(padded_reference_ids, dim=0)\n",
    "    padded_reference_attention_mask = [item[1] for item in padded_input_attention_ids]\n",
    "    padded_reference_attention_mask = torch.concatenate(padded_reference_attention_mask, dim=0)\n",
    "    padded_position_ids = [auto_padding(item, real_max_chunk_size, filling_value=0) for item in position_input_ids]\n",
    "    padded_reference_position_ids = torch.concatenate(padded_position_ids, dim=0)\n",
    "\n",
    "    # Create Question\n",
    "    question = QUESTION_TEMPLATE.format(question=combined_question)\n",
    "    tok_question = tokenizer(question, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    padded_tok_question, padded_question_attention_mask = auto_padding(tok_question[0], tok_question.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    question_position_input_ids = create_position_ids(tok_question.size(-1), qa_size) + max_embedding_size - qa_size \n",
    "\n",
    "    # Create Chosen / Rejected Answers / and their labels\n",
    "    chosen_answer = ANSWER_TEMPLATE.format(answer=combined_answer)\n",
    "    prefix_rejected_answer = ANSWER_TEMPLATE.format(answer=prefix_a)\n",
    "    suffix_rejected_answer = ANSWER_TEMPLATE.format(answer=suffix_a)\n",
    "    tok_chosen_answer = tokenizer(chosen_answer, return_tensors=\"pt\").input_ids[0][1:]\n",
    "    tok_prefix_rejected_answer = tokenizer(prefix_rejected_answer, return_tensors=\"pt\").input_ids[0][1:]\n",
    "    tok_suffix_rejected_answer = tokenizer(suffix_rejected_answer, return_tensors=\"pt\").input_ids[0][1:]\n",
    "\n",
    "    system_reference_question_size = max_embedding_size - qa_size + padded_question_attention_mask.size(-1)\n",
    "\n",
    "    padded_tok_chosen_answer, padded_chosen_answer_attention_mask = auto_padding(\n",
    "        tok_chosen_answer, qa_size - padded_question_attention_mask.size(-1), \n",
    "        filling_value=0, return_attention_mask=True\n",
    "    )\n",
    "    tok_chosen_answer_labels = auto_padding(tok_chosen_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    chosen_answer_position_ids = create_position_ids(tok_chosen_answer.size(-1), tok_chosen_answer.size(-1)) + system_reference_question_size\n",
    "    chosen_answer_position_ids = auto_padding(chosen_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "\n",
    "    padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask = auto_padding(\n",
    "        tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_prefix_rejected_answer_labels = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    prefix_rejected_answer_position_ids = create_position_ids(tok_prefix_rejected_answer.size(-1), tok_prefix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    prefix_rejected_answer_position_ids = auto_padding(prefix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "    \n",
    "    padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask = auto_padding(\n",
    "        tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_suffix_rejected_answer_labels = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    suffix_rejected_answer_position_ids = create_position_ids(tok_suffix_rejected_answer.size(-1), tok_suffix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    suffix_rejected_answer_position_ids = auto_padding(\n",
    "        suffix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "\n",
    "    # Merge All the Inputs Data\n",
    "    concatenated_batch = {}\n",
    "    concatenated_batch[\"input_ids\"] = torch.concatenate([padded_tok_suffix, padded_reference_ids, padded_tok_question], dim=0)\n",
    "    concatenated_batch[\"attention_mask\"] = torch.concatenate([padded_suffix_attention_mask, padded_reference_attention_mask, padded_question_attention_mask], dim=0)\n",
    "    concatenated_batch[\"position_ids\"] = torch.concatenate([system_position_ids, padded_reference_position_ids, question_position_input_ids], dim=0)\n",
    "    referece_question_length = concatenated_batch[\"attention_mask\"].size(-1)\n",
    "    referece_question_labels = torch.full((1, referece_question_length), -100)[0]\n",
    "    \n",
    "    # Create Labels for Each Part\n",
    "    concatenated_batch[\"chosen_answer\"] = {\n",
    "        \"input_ids\": padded_tok_chosen_answer, \n",
    "        \"attention_mask\": padded_chosen_answer_attention_mask, \n",
    "        \"labels\": torch.concatenate([referece_question_labels, tok_chosen_answer_labels], dim=0),\n",
    "        \"position_ids\": chosen_answer_position_ids\n",
    "    }\n",
    "    concatenated_batch[\"prefix_rejected_answer\"] = {\n",
    "        \"input_ids\": padded_tok_prefix_rejected_answer, \n",
    "        \"attention_mask\": padded_prefix_rejected_answer_attention_mask, \n",
    "        \"labels\": torch.concatenate([referece_question_labels, tok_prefix_rejected_answer_labels], dim=0),\n",
    "        \"position_ids\": prefix_rejected_answer_position_ids,\n",
    "    }\n",
    "    concatenated_batch[\"suffix_rejected_answer\"] = {\n",
    "        \"input_ids\": padded_tok_suffix_rejected_answer, \n",
    "        \"attention_mask\": padded_suffix_rejected_answer_attention_mask, \n",
    "        \"labels\": torch.concatenate([referece_question_labels, tok_suffix_rejected_answer_labels], dim=0),\n",
    "        \"position_ids\": suffix_rejected_answer_position_ids,\n",
    "    }\n",
    "\n",
    "    return concatenated_batch, sum(statistic_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19])\n",
      "torch.Size([19])\n",
      "torch.Size([19])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n",
      "torch.Size([81])\n"
     ]
    }
   ],
   "source": [
    "def combine_fn(lst, max_candidates=2):\n",
    "    trimmed_lists = [random.sample(sublst, min(len(sublst), max_candidates)) if len(sublst) > max_candidates else sublst for sublst in lst]\n",
    "    all_combinations = itertools.product(*trimmed_lists)\n",
    "    concatenated_results = [torch.cat(combination) for combination in all_combinations]\n",
    "    return concatenated_results\n",
    "\n",
    "def create_system_suffix(tokenizer, system_suffix, special_token_id: int=13):\n",
    "    tok_suffix = tokenizer(system_suffix, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    padded_tok_suffix, padded_suffix_attention_mask = tok_suffix.input_ids[0], tok_suffix.attention_mask[0]\n",
    "    # add special token\n",
    "    padded_tok_suffix = torch.concatenate([padded_tok_suffix, torch.tensor([special_token_id])], dim=0)\n",
    "    padded_suffix_attention_mask = torch.concatenate([padded_suffix_attention_mask, torch.tensor([1])], dim=0)\n",
    "    system_position_ids = torch.arange(0, padded_tok_suffix.size(-1))\n",
    "    return padded_tok_suffix, padded_suffix_attention_mask, system_position_ids\n",
    "\n",
    "def create_chunked_reference(tokenizer: AutoTokenizer, all_refs: List[str], real_reference_size: int, max_embedding_size: int, system_prompt_size: int, qa_size: int, special_token_id: int=13):\n",
    "    real_max_chunk_size = real_reference_size // len(all_refs) - 1 # allocate one position for attention reallocation\n",
    "    tok_all_ref = [tokenizer(item, return_tensors=\"pt\", add_special_tokens=False).input_ids[0] for item in all_refs]\n",
    "    truncted_refer_tok_lst, statistic_data_size = [], []\n",
    "    for item in tok_all_ref:\n",
    "        statistic_data_size.append(item.size(-1))\n",
    "        if item.size(-1) > real_max_chunk_size: \n",
    "            item = item[: real_max_chunk_size]\n",
    "        truncted_refer_tok_lst.append(item)\n",
    "\n",
    "    fake_position_chunk_size = real_max_chunk_size + 1  # with last special token index for each chunk\n",
    "    positional_chunks = torch.arange(system_prompt_size, max_embedding_size - qa_size, fake_position_chunk_size)\n",
    "    # Here, end_positional_chunks denotes special token ids\n",
    "    begin_positional_chunks, end_positional_chunks = positional_chunks[:-1], positional_chunks[1:] - 1  \n",
    "    all_chunk_pos_lst = []\n",
    "    \n",
    "    for i, item in enumerate(truncted_refer_tok_lst):\n",
    "        chunk_token_pos_lst = create_covering_position_ids(item.size(-1), real_max_chunk_size)\n",
    "        chunk_token_pos_lst = [item + begin_positional_chunks[i] for item in chunk_token_pos_lst]\n",
    "        all_chunk_pos_lst.append(chunk_token_pos_lst)\n",
    "\n",
    "    padded_chunk_pos_lst = [[auto_padding(sub_item, real_max_chunk_size, filling_value=0, return_attention_mask=False) for sub_item in item] for item in all_chunk_pos_lst]\n",
    "    padded_refer_tok_lst = [auto_padding(item, real_max_chunk_size, filling_value=0, return_attention_mask=True) for item in truncted_refer_tok_lst]\n",
    "    padded_refer_tok_ids = [item[0] for item in padded_refer_tok_lst]\n",
    "    padded_refer_attention_mask = [item[1] for item in padded_refer_tok_lst]\n",
    "\n",
    "    candicated_padded_position_ids = []\n",
    "    padded_ref_input_ids_lst, padded_ref_attention_mask_lst = [], []\n",
    "    \n",
    "    for chunk_pos_ids, chunk_spe_pos_lst in zip(end_positional_chunks, padded_chunk_pos_lst):\n",
    "        tmp_chunk_pos_ids = []\n",
    "        for tmp in chunk_spe_pos_lst:\n",
    "            tmp = torch.concatenate([tmp, torch.tensor([chunk_pos_ids])], dim=0)\n",
    "            tmp_chunk_pos_ids.append(tmp)  # [[0,1,...,C1], [C2,C2+1,...,C3], ...]\n",
    "        candicated_padded_position_ids.append(tmp_chunk_pos_ids)\n",
    "    candicated_padded_position_ids = combine_fn(candicated_padded_position_ids)\n",
    "\n",
    "    for padded_chunk_tok_ref_input_ids, padded_chunk_tok_ref_attention_mask in zip(padded_refer_tok_ids, padded_refer_attention_mask):\n",
    "        padded_chunk_tok_ref_input_ids = torch.concatenate([padded_chunk_tok_ref_input_ids, torch.tensor([special_token_id])], dim=0)\n",
    "        padded_chunk_tok_ref_attention_mask = torch.concatenate([padded_chunk_tok_ref_attention_mask, torch.tensor([1])], dim=0)\n",
    "        padded_ref_input_ids_lst.append(padded_chunk_tok_ref_input_ids)\n",
    "        padded_ref_attention_mask_lst.append(padded_chunk_tok_ref_attention_mask)\n",
    "    \n",
    "    padded_ref_input_ids = torch.concatenate(padded_ref_input_ids_lst, dim=0)\n",
    "    padded_ref_attention_mask = torch.concatenate(padded_ref_attention_mask_lst, dim=0)\n",
    "    all_spe_pos = torch.arange(real_max_chunk_size, real_reference_size, real_max_chunk_size + 1)\n",
    "\n",
    "    return candicated_padded_position_ids, padded_ref_input_ids, padded_ref_attention_mask, all_spe_pos\n",
    "\n",
    "\n",
    "def create_qa(QUESTION_TEMPLATE, ANSWER_TEMPLATE, combined_question, combined_answer, prefix_a: str, suffix_a: str, last_position: int, qa_size: int, special_token_id: int):\n",
    "    \"\"\"\n",
    "    last_position 是reference position ids最大的数值，下面的代码要加一个 last_position + 1的shift\n",
    "    qa_size 规定了最大的qa 的长度，所以总长度需要手动卡一下\n",
    "    \"\"\"\n",
    "    # Create Question\n",
    "    question = QUESTION_TEMPLATE.format(question=combined_question)\n",
    "    tok_question = tokenizer(question, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    padded_tok_question, padded_question_attention_mask = auto_padding(tok_question, tok_question.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    padded_tok_question = torch.concatenate([padded_tok_question, torch.tensor([special_token_id])], dim=0)\n",
    "    padded_question_attention_mask = torch.concatenate([padded_question_attention_mask, torch.tensor([1])], dim=0)\n",
    "    question_position_input_ids = create_position_ids(tok_question.size(-1), tok_question.size(-1)) + last_position + 1\n",
    "    last_pos = question_position_input_ids.max() + 1\n",
    "    question_position_input_ids = torch.concatenate([question_position_input_ids, torch.tensor([last_pos])], dim=0)\n",
    "    spe_tok_pos = question_position_input_ids.size(-1) - 1\n",
    "\n",
    "    # Create Chosen / Rejected Answers / and their labels\n",
    "    chosen_answer = ANSWER_TEMPLATE.format(answer=combined_answer)\n",
    "    prefix_rejected_answer = ANSWER_TEMPLATE.format(answer=prefix_a)\n",
    "    suffix_rejected_answer = ANSWER_TEMPLATE.format(answer=suffix_a)\n",
    "    tok_chosen_answer = tokenizer(chosen_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    tok_prefix_rejected_answer = tokenizer(prefix_rejected_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    tok_suffix_rejected_answer = tokenizer(suffix_rejected_answer, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    system_reference_question_size = last_position + 1 + question_position_input_ids.size(-1)\n",
    "\n",
    "    padded_tok_chosen_answer, padded_chosen_answer_attention_mask = auto_padding(tok_chosen_answer, qa_size-padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_chosen_answer_labels = auto_padding(tok_chosen_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    chosen_answer_position_ids = create_position_ids(tok_chosen_answer.size(-1), tok_chosen_answer.size(-1)) + system_reference_question_size\n",
    "    chosen_answer_position_ids = auto_padding(chosen_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "\n",
    "    padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_prefix_rejected_answer_labels = auto_padding(tok_prefix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    prefix_rejected_answer_position_ids = create_position_ids(tok_prefix_rejected_answer.size(-1), tok_prefix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    prefix_rejected_answer_position_ids = auto_padding(prefix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    "    \n",
    "    padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=0, return_attention_mask=True)\n",
    "    tok_suffix_rejected_answer_labels = auto_padding(tok_suffix_rejected_answer, qa_size - padded_question_attention_mask.size(-1), filling_value=-100)\n",
    "    suffix_rejected_answer_position_ids = create_position_ids(tok_suffix_rejected_answer.size(-1), tok_suffix_rejected_answer.size(-1)) + system_reference_question_size\n",
    "    suffix_rejected_answer_position_ids = auto_padding(suffix_rejected_answer_position_ids, qa_size - padded_question_attention_mask.size(-1), filling_value=0)\n",
    " \n",
    "    return padded_tok_question, padded_question_attention_mask, question_position_input_ids, \\\n",
    "        padded_tok_chosen_answer, padded_chosen_answer_attention_mask, tok_chosen_answer_labels, chosen_answer_position_ids, \\\n",
    "        padded_tok_prefix_rejected_answer, padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, prefix_rejected_answer_position_ids, \\\n",
    "        padded_tok_suffix_rejected_answer, padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, suffix_rejected_answer_position_ids, spe_tok_pos\n",
    "    \n",
    "\n",
    "\"\"\"block testing create_chunked_reference\"\"\" \n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/vepfs/wcf/hf_models/Meta-Llama-3-8B-Instruct\")\n",
    "all_refs = [\"hello, world\", \"Any, iowpq\", \"reason medsa\"]\n",
    "real_reference_size = 28\n",
    "system_prompt_size, qa_size = 2, 4\n",
    "max_embedding_size = 34\n",
    "candicated_padded_concat_position_ids, padded_ref_input_ids, padded_ref_attention_mask, all_spe_pos = create_chunked_reference(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size)\n",
    "\n",
    "\"\"\"Test Create QA Function\"\"\"\n",
    "QUESTION_TEMPLATE = \"<|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the following question according to the references: {question}<|eot_id|>\"\n",
    "ANSWER_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\\n\\nThe answer is: {answer}<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "padded_tok_question, padded_question_attention_mask, \\\n",
    "question_position_input_ids, padded_tok_chosen_answer, \\\n",
    "padded_chosen_answer_attention_mask, tok_chosen_answer_labels, \\\n",
    "chosen_answer_position_ids, padded_tok_prefix_rejected_answer, \\\n",
    "padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, \\\n",
    "prefix_rejected_answer_position_ids, padded_tok_suffix_rejected_answer, \\\n",
    "padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, \\\n",
    "suffix_rejected_answer_position_ids, spe_tok_pos = create_qa(QUESTION_TEMPLATE, ANSWER_TEMPLATE, \"who are you\", \"jack\", \"prefix_a\", \"suffix_a\", last_position=1024, qa_size=100, special_token_id=13)\n",
    "\n",
    "print(padded_tok_question.shape)\n",
    "print(padded_question_attention_mask.shape)\n",
    "print(question_position_input_ids.shape)\n",
    "print(padded_tok_chosen_answer.shape)\n",
    "print(chosen_answer_position_ids.shape)\n",
    "print(prefix_rejected_answer_position_ids.shape)\n",
    "print(padded_tok_suffix_rejected_answer.shape)\n",
    "print(padded_suffix_rejected_answer_attention_mask.shape)\n",
    "print(suffix_rejected_answer_position_ids.shape)\n",
    "print(tok_suffix_rejected_answer_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candicated_padded_concat_position_ids[0])\n",
    "print(padded_ref_input_ids)\n",
    "print(padded_ref_attention_mask)\n",
    "print(all_spe_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'prefix_q', 'prefix_a', 'suffix_q', 'suffix_a', 'prefix_id', 'suffix_id', 'all_ref_ids', 'combined_question', 'all_ref_text', 'final_answer'],\n",
      "    num_rows: 878\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Current Avg Seq Length: 16436.81: 100%|██████████| 878/878 [00:39<00:00, 22.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def find_index(id_lst, prefix_id, suffix_id):\n",
    "    return id_lst.index(prefix_id), id_lst.index(suffix_id)\n",
    "\n",
    "def create_covering_position_ids(N, L):\n",
    "    \"\"\"Create sets of position IDs to cover all positions from 0 to L-1 with intervals of length N.\"\"\"\n",
    "    if N > L:\n",
    "        raise ValueError(\"N should not be greater than L\")\n",
    "    num_intervals = (L + N - 1) // N\n",
    "    position_ids_list = []\n",
    "    for i in range(num_intervals):\n",
    "        start_pos = i * (L - N) // (num_intervals - 1) if num_intervals > 1 else 0\n",
    "        end_pos = start_pos + N\n",
    "        if end_pos > L:\n",
    "            end_pos = L\n",
    "            start_pos = L - N if L > N else 0\n",
    "        position_ids = torch.arange(start_pos, end_pos)\n",
    "        position_ids_list.append(position_ids)\n",
    "    return position_ids_list\n",
    "\n",
    "\n",
    "def auto_padding(t: torch.Tensor, length: int, filling_value=-100, return_attention_mask=False):\n",
    "    if length < t.size(0):\n",
    "        if return_attention_mask: return t[:length]\n",
    "        else: return t[:length], torch.ones_like(t[:length])\n",
    "    padded_tensor = torch.full((length,), filling_value, dtype=t.dtype)\n",
    "    padded_tensor[:t.size(0)] = t\n",
    "    if return_attention_mask:\n",
    "        attention_mask = torch.zeros(length, dtype=torch.int)\n",
    "        attention_mask[:t.size(0)] = 1\n",
    "        return padded_tensor, attention_mask\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "def create_covering_position_ipt_data(tokenizer, all_refs: List[str], combined_question: str, combined_answer: str, prefix_a: str, suffix_a: str, qa_size: int, max_embedding_size: int, real_reference_size: int, special_token_id: int = None, prefix_id: int = None, suffix_id: int = None):\n",
    "    statistic_data_size = []\n",
    "\n",
    "    SYSTEM_SUFFIX = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nBelow is some references. Please read it carefully and answer the following question.<|eot_id|>\"\n",
    "    QUESTION_TEMPLATE = \"<|start_header_id|>user<|end_header_id|>\\n\\nPlease answer the following question according to the references: {question}<|eot_id|>\"\n",
    "    ANSWER_TEMPLATE = \"<|start_header_id|>assistant<|end_header_id|>\\n\\nThe answer is: {answer}<|eot_id|><|end_of_text|>\"\n",
    "\n",
    "    # Create System Suffix\n",
    "    padded_tok_input_ids_system_suffix, padded_attention_mask_system_suffix, padded_position_ids_system_suffix = create_system_suffix(tokenizer, SYSTEM_SUFFIX, special_token_id)\n",
    "    system_prompt_size = padded_attention_mask_system_suffix.size(-1)\n",
    "    all_spe_pos = [system_prompt_size-1]\n",
    "    # create chunk reference (input_ids, attention_mask and positional ids)\n",
    "    candicated_padded_position_ids_lst, padded_ref_input_ids, padded_ref_attention_mask, ref_spe_pos = create_chunked_reference(tokenizer, all_refs, real_reference_size, max_embedding_size, system_prompt_size, qa_size, special_token_id)\n",
    "\n",
    "    ref_spe_pos += system_prompt_size\n",
    "    all_spe_pos.extend(ref_spe_pos.tolist())\n",
    "\n",
    "    # combine and wrap each position_id, input_ids and attention_mask\n",
    "    last_position = max_embedding_size - qa_size  # size for real reference and system prompt\n",
    "\n",
    "    # Create Question, all Answers\n",
    "    padded_tok_question, padded_question_attention_mask, \\\n",
    "    question_position_input_ids, padded_tok_chosen_answer, \\\n",
    "    padded_chosen_answer_attention_mask, tok_chosen_answer_labels, \\\n",
    "    chosen_answer_position_ids, padded_tok_prefix_rejected_answer, \\\n",
    "    padded_prefix_rejected_answer_attention_mask, tok_prefix_rejected_answer_labels, \\\n",
    "    prefix_rejected_answer_position_ids, padded_tok_suffix_rejected_answer, \\\n",
    "    padded_suffix_rejected_answer_attention_mask, tok_suffix_rejected_answer_labels, \\\n",
    "    suffix_rejected_answer_position_ids, spe_tok_pos = create_qa(\n",
    "        QUESTION_TEMPLATE, ANSWER_TEMPLATE, combined_question, combined_answer, prefix_a, suffix_a, last_position, qa_size, special_token_id=special_token_id\n",
    "    )\n",
    "    all_spe_pos.append(spe_tok_pos + system_prompt_size + padded_ref_input_ids.size(-1))\n",
    "    # all_spe_pos = torch.concatenate([all_spe_pos, torch.tensor([spe_tok_pos])], dim=0)\n",
    "\n",
    "    # Merge All the Inputs Data\n",
    "    all_datasets = []  # different combination of positions \n",
    "    # if len(candicated_padded_position_ids_lst) > 8:\n",
    "    #     print(len(candicated_padded_position_ids_lst))\n",
    "    #     print(len(all_refs))\n",
    "    for i, ref_position_id in enumerate(candicated_padded_position_ids_lst):\n",
    "        concatenated_batch = {}\n",
    "        concatenated_batch[\"input_ids\"] = torch.concatenate([padded_tok_input_ids_system_suffix, padded_ref_input_ids, padded_tok_question], dim=0)\n",
    "        concatenated_batch[\"attention_mask\"] = torch.concatenate([padded_attention_mask_system_suffix, padded_ref_attention_mask, padded_question_attention_mask], dim=0)\n",
    "        concatenated_batch[\"position_ids\"] = torch.concatenate([padded_position_ids_system_suffix, ref_position_id, question_position_input_ids], dim=0)\n",
    "        referece_question_length = concatenated_batch[\"attention_mask\"].size(-1)\n",
    "        concatenated_batch[\"all_spe_pos\"] = all_spe_pos\n",
    "        referece_question_labels = torch.full((1, referece_question_length), -100)[0]\n",
    "        \n",
    "        # Create Labels for Each Part\n",
    "        concatenated_batch[\"chosen_answer\"] = {\n",
    "            \"input_ids\": padded_tok_chosen_answer, \n",
    "            \"attention_mask\": padded_chosen_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_chosen_answer_labels], dim=0),\n",
    "            \"position_ids\": chosen_answer_position_ids\n",
    "        }\n",
    "        concatenated_batch[\"prefix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_prefix_rejected_answer, \n",
    "            \"attention_mask\": padded_prefix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_prefix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": prefix_rejected_answer_position_ids,\n",
    "        }\n",
    "        concatenated_batch[\"suffix_rejected_answer\"] = {\n",
    "            \"input_ids\": padded_tok_suffix_rejected_answer, \n",
    "            \"attention_mask\": padded_suffix_rejected_answer_attention_mask, \n",
    "            \"labels\": torch.concatenate([referece_question_labels, tok_suffix_rejected_answer_labels], dim=0),\n",
    "            \"position_ids\": suffix_rejected_answer_position_ids,\n",
    "        }\n",
    "        concatenated_batch[\"chosen_ids\"] = (prefix_id, suffix_id)\n",
    "        all_datasets.append(concatenated_batch)\n",
    "        statistic_data_size.append(concatenated_batch[\"input_ids\"].size(-1))\n",
    "\n",
    "    return all_datasets, sum(statistic_data_size) / len(statistic_data_size)\n",
    "\n",
    "\n",
    "dataset = datasets.load_from_disk(\"/vepfs/wcf/G/zecheng/data/hf_dataset_step2\")\n",
    "print(dataset)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/vepfs/wcf/G/zecheng/hf_models2/Meta-Llama-3-8B-Instruct\")\n",
    "training_samples = []\n",
    "avg_real_seq_length = 0\n",
    "spe_token_id = tokenizer(\"<|reserved_special_token_0|>\", add_special_tokens=False).input_ids[0]\n",
    "with tqdm(total=len(dataset), desc=f\"Initial Avg Length: {avg_real_seq_length}\") as pbar:\n",
    "    for item in dataset:\n",
    "        all_ref_text = item[\"all_ref_text\"]\n",
    "        combined_question, final_answer = item[\"combined_question\"], item[\"final_answer\"]\n",
    "        prefix_q, suffix_q = item[\"prefix_q\"], item[\"suffix_q\"]\n",
    "        prefix_a, suffix_a = item[\"prefix_a\"], item[\"suffix_a\"]\n",
    "        prefix_id, suffix_id = find_index(item[\"all_ref_ids\"], item[\"prefix_id\"], item[\"suffix_id\"])\n",
    "        all_datasets, ref_length = create_covering_position_ipt_data(tokenizer, all_ref_text, combined_question, final_answer, prefix_a, suffix_a, qa_size=512, max_embedding_size=65536, real_reference_size=16384, special_token_id=spe_token_id, prefix_id=prefix_id, suffix_id=suffix_id)\n",
    "        avg_real_seq_length += ref_length / len(dataset)\n",
    "        training_samples.extend(all_datasets)\n",
    "        pbar.set_description(f\"Current Avg Seq Length: {avg_real_seq_length:.2f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "print(len(training_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_length = 0\n",
    "with tqdm(total=len(dataset)) as pbar:\n",
    "    for sample in dataset:\n",
    "        all_length += sum([tokenizer(item, return_tensors=\"pt\", add_special_tokens=False).input_ids.size(-1) for item in sample[\"all_ref_text\"]]) / len(dataset)\n",
    "        pbar.update(1)\n",
    "print(all_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelzipper import *\n",
    "# import pandas as pd\n",
    "# import datasets\n",
    "# import numpy as np\n",
    "\n",
    "converted_dict = {}\n",
    "for item in training_samples:\n",
    "    for key, value in item.items():\n",
    "        if key in converted_dict:\n",
    "            converted_dict[key].append(value)\n",
    "        else:\n",
    "            converted_dict[key] = [value]\n",
    "\n",
    "hf_datasets = datasets.Dataset.from_dict(converted_dict)\n",
    "hf_datasets.save_to_disk(\"/vepfs/wcf/G/zecheng/data/processed_data/hf_dataset_8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (19/19 shards): 100%|██████████| 14040/14040 [00:03<00:00, 4472.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_datasets.save_to_disk(\"/vepfs/wcf/G/zecheng/data/processed_data/hf_dataset_tmp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "hf_datasets = datasets.load_from_disk(\"/data/zecheng/data/process_wiki_document/two_hop/hf_dataset_tmp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_datasets)\n",
    "print(hf_datasets[0]['all_spe_pos'])\n",
    "print(hf_datasets[0]['chosen_ids'])\n",
    "for idx in hf_datasets[0]['all_spe_pos']:\n",
    "    print(idx)\n",
    "    print(hf_datasets[0]['attention_mask'][idx-20: idx+1])\n",
    "    print(hf_datasets[0]['input_ids'][idx-20: idx+1])\n",
    "    print(tokenizer.decode(hf_datasets[0]['input_ids'][idx-20: idx+1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
