{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "import re\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "import random\n",
    "import transformers\n",
    "from datasets import Dataset, concatenate_datasets, DatasetDict, load_from_disk, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_NUM = 64\n",
    "\n",
    "def extract_qa_pairs(text):\n",
    "    sp_text = text.split(\"####\")\n",
    "    if len(sp_text) < 9:\n",
    "        return None\n",
    "    q1, a1, q2, a2 = sp_text[2].strip(), sp_text[4].strip(), sp_text[6].strip(), sp_text[8].strip()\n",
    "    # pattern = r\"####Question \\d+####(.*?)####Answer \\d+####(.*?)(?=####Question \\d+####|$)\"\n",
    "    # matches = re.findall(pattern, text, re.DOTALL)\n",
    "   \n",
    "    res = [{\"question\": q1, \"answer\": a1}, {\"question\": q2, \"answer\": a2}]\n",
    "    # for i, match in enumerate(matches, 1):\n",
    "    #     question_text, answer_text = match\n",
    "    #     res.append({\"question\": question_text, \"answer\": answer_text})\n",
    "    return res\n",
    "\n",
    "def combine_data(data, chunk_num):\n",
    "    combined_data = []\n",
    "    for i in range(0, len(data), chunk_num):\n",
    "        combined_data.append(data[i:i+chunk_num])\n",
    "    return combined_data\n",
    "\n",
    "all_data = load_from_disk(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/filtering_deepseek/filtered_wo_ana\")\n",
    "\n",
    "# files = auto_read_dir(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B\", file_prefix=\"generated_QA_pairs_thread\", file_suffix=\".jsonl\")\n",
    "# all_data = [auto_read_data(file) for file in files]\n",
    "# all_data = [item for sublist in all_data for item in sublist]\n",
    "\n",
    "print(f\"total pairs: {len(all_data)}\")\n",
    "\n",
    "processed_data = []\n",
    "for item in all_data:\n",
    "    ref, qa_pairs = item['reference'], item['qa_pairs']\n",
    "    processed_data.append({\"reference\": ref, \"qa_pairs\": qa_pairs})\n",
    "\n",
    "combined_data = combine_data(processed_data, chunk_num=CHUNK_NUM)\n",
    "print(len(combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建强化学习的数据\n",
    "\n",
    "\n",
    "#### DPO 数据格式\n",
    "return {\n",
    "        \"prompt\": [\"Question: \" + question + \"\\n\\nAnswer: \" for question in samples[\"question\"]],\n",
    "        \"chosen\": samples[\"response_j\"],\n",
    "        \"rejected\": samples[\"response_k\"],\n",
    "    }\n",
    "\n",
    "##### 1) 任意格式数据构建\n",
    "潜在问题: attention可能会直接进行attribution matching，而不是fuss matching\n",
    "\n",
    "\n",
    "##### 2) 更加细粒度的数据构建模式\n",
    "潜在问题：构建规则的判断，比如KNN等聚类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select_from_combined_data(all_samples, num_cases=8, selected_cases=1):\n",
    "    SEP_TOKEN = ' [Doc] '\n",
    "    # TEMPLATE = \"{reference}\\n\\nQuestions: {question}\"\n",
    "    cases = list(range(num_cases))\n",
    "    combinations_list = list(itertools.combinations(cases, selected_cases))\n",
    "\n",
    "    batch_data = []\n",
    "    ref_lst = [item['reference'] for item in all_samples]\n",
    "    \n",
    "    for item in combinations_list:\n",
    "        chosen_id = item[0]\n",
    "        remain_case_ids = list(set(cases) - set((chosen_id,)))\n",
    "        reject_id = random.choice(remain_case_ids)\n",
    "        for i in range(len(all_samples[chosen_id])):\n",
    "            for j in range(len(all_samples[reject_id])):\n",
    "                question = all_samples[chosen_id]['qa_pairs'][i]['question']\n",
    "                cur_sample = {\n",
    "                    \"reference_list\": ref_lst, \n",
    "                    \"question\": question,\n",
    "                    # \"prompt\": TEMPLATE.format(reference=references, question=question), \n",
    "                    \"chosen\": all_samples[chosen_id]['qa_pairs'][i]['answer'], \n",
    "                    \"rejected\": all_samples[reject_id]['qa_pairs'][j]['answer'],\n",
    "                    \"chosen_span_id\": chosen_id, \n",
    "                    \"rejected_span_id\": reject_id,\n",
    "                }\n",
    "                batch_data.append(cur_sample)\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "all_created_cases = []\n",
    "\n",
    "with tqdm(total=len(combined_data)) as pbar:\n",
    "    for c_data in combined_data:\n",
    "        batch_data = random_select_from_combined_data(c_data, num_cases=len(c_data), selected_cases=1)\n",
    "        all_created_cases += batch_data\n",
    "        pbar.update(1)\n",
    "\n",
    "print(f\"finish, current data sample nums: {len(all_created_cases)}\")\n",
    "\n",
    "print(all_created_cases[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = all_created_cases[500:], all_created_cases[:500]\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "valid_df = pd.DataFrame(valid_dataset)\n",
    "trans_train_datasets = Dataset.from_pandas(train_df, split=\"train\")\n",
    "trans_valid_datasets = Dataset.from_pandas(valid_df, split=\"valid\")\n",
    "\n",
    "combined_datasets = DatasetDict({\"train\": trans_train_datasets, \"valid\": trans_valid_datasets})\n",
    "save_path = f\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_{CHUNK_NUM}\"\n",
    "combined_datasets.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_datasets['valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"/vepfs/wcf/G/zecheng/hf_models/Mistral-7B-Instruct-v0.2\")\n",
    "# dataset = auto_read_data(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/dpo_data_v1.jsonl\")\n",
    "\n",
    "def convert_format(tmp):\n",
    "    prompt, chosen, rejected = tmp[\"prompt\"], tmp[\"chosen\"], tmp[\"rejected\"]\n",
    "    # user_message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # user_message = tokenizer.apply_chat_template(user_message, tokenize=False)\n",
    "    chosen_span_id, rejected_span_id = tmp['chosen_span_id'], tmp['rejected_span_id']\n",
    "    return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": rejected,\n",
    "            \"chosen_span_id\": chosen_span_id,\n",
    "            \"rejected_span_id\": rejected_span_id,\n",
    "        }\n",
    "\n",
    "transfer_datasets = [convert_format(item) for item in all_created_cases]\n",
    "train_datasets, valid_datasets = transfer_datasets[500:], transfer_datasets[:500]\n",
    "\n",
    "# auto_save_data(train_datasets, \"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/dpo_data_chat_train_v2.jsonl\")\n",
    "# auto_save_data(valid_datasets, \"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/dpo_data_chat_valid_v2.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/vepfs/wcf/G/zecheng/hf_models/Llama-2-7b-hf\")\n",
    "\n",
    "# train_datasets = pd.read_json(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/dpo_data_chat_train_v2.jsonl\", lines=True)\n",
    "# valid_datasets = pd.read_json(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/dpo_data_chat_valid_v2.jsonl\", lines=True)\n",
    "# trans_train_datasets = Dataset.from_dict(train_datasets)\n",
    "\n",
    "train_df = pd.DataFrame(train_datasets)\n",
    "valid_df = pd.DataFrame(valid_datasets)\n",
    "\n",
    "# print(train_df.head())\n",
    "# print(valid_df.head())\n",
    "\n",
    "trans_train_datasets = Dataset.from_pandas(train_datasets, split=\"train\")\n",
    "trans_valid_datasets = Dataset.from_pandas(valid_datasets, split=\"valid\")\n",
    "\n",
    "combined_datasets = DatasetDict({\"train\": trans_train_datasets, \"valid\": trans_valid_datasets})\n",
    "# all_data = load_from_disk(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_v2\")\n",
    "\n",
    "train_data = combined_datasets['train']\n",
    "valid_data = combined_datasets['valid']\n",
    "print(train_data)\n",
    "print(valid_data)\n",
    "\n",
    "# total_length = 0\n",
    "# max_length = 0\n",
    "# min_length = 1e5\n",
    "\n",
    "def map_func(sample):\n",
    "    tok_seq = tokenizer(sample['prompt'], add_special_tokens=False)['input_ids']\n",
    "    cur_length = len(tok_seq)\n",
    "    sample[\"cur_length\"] = cur_length\n",
    "    return sample\n",
    "    \n",
    "process__data = combined_datasets.map(map_func, num_proc=24)\n",
    "\n",
    "for k in ['train', 'valid']:\n",
    "    print(f'-------- key: {k} --------')\n",
    "    cur_data = process__data[k]\n",
    "    lengths = cur_data[\"cur_length\"]\n",
    "\n",
    "    # 将少于 5000 的长度设为 5000，将多于 12000 的长度设为 12000\n",
    "    lengths = np.clip(lengths, 5000, 12000)\n",
    "\n",
    "    # 定义区间\n",
    "    bins = np.arange(5000, 13001, 1000)\n",
    "\n",
    "    # 计算每个区间的样本数量\n",
    "    hist, bin_edges = np.histogram(lengths, bins=bins)\n",
    "\n",
    "    # 计算每个区间的比例\n",
    "    total_samples = len(lengths)\n",
    "    proportions = hist / total_samples \n",
    "\n",
    "    # 打印结果\n",
    "    for i in range(len(bins) - 1):\n",
    "        print(f\"Range {bins[i]} - {bins[i+1]}: {proportions[i]:.2%}\")\n",
    "\n",
    "def filter_func(sample):\n",
    "    return sample[\"cur_length\"] <= 16000  # filter with 16K length\n",
    "\n",
    "filtered_train_data = process__data['train'].filter(filter_func, num_proc=24)\n",
    "filtered_valid_data = process__data['valid'].filter(filter_func, num_proc=24)\n",
    "filtered_train_data = filtered_train_data.remove_columns([\"cur_length\"])\n",
    "filtered_valid_data = filtered_valid_data.remove_columns([\"cur_length\"])\n",
    "filtered_combined_datasets = DatasetDict({\"train\": filtered_train_data, \"valid\": filtered_valid_data})\n",
    "\n",
    "save_path = f\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_{CHUNK_NUM}\"\n",
    "filtered_combined_datasets.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 备份代码\n",
    "\n",
    "备份代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['train', 'valid']:\n",
    "    print(f'-------- key: {k} --------')\n",
    "    cur_data = process__data[k]\n",
    "    lengths = cur_data[\"cur_length\"]\n",
    "\n",
    "    # 将少于 5000 的长度设为 5000，将多于 12000 的长度设为 12000\n",
    "    lengths = np.clip(lengths, 5000, 12000)\n",
    "\n",
    "    # 定义区间\n",
    "    bins = np.arange(5000, 13001, 1000)\n",
    "\n",
    "    # 计算每个区间的样本数量\n",
    "    hist, bin_edges = np.histogram(lengths, bins=bins)\n",
    "\n",
    "    # 计算每个区间的比例\n",
    "    total_samples = len(lengths)\n",
    "    proportions = hist / total_samples\n",
    "\n",
    "    # 打印结果\n",
    "    for i in range(len(bins) - 1):\n",
    "        print(f\"Range {bins[i]} - {bins[i+1]}: {proportions[i]:.2%}\")\n",
    "\n",
    "def filter_func(sample):\n",
    "    return sample[\"cur_length\"] <= 8000\n",
    "\n",
    "filtered_train_data = process__data['train'].filter(filter_func, num_proc=36)\n",
    "filtered_valid_data = process__data['valid'].filter(filter_func, num_proc=36)\n",
    "filtered_train_data = filtered_train_data.remove_columns([\"cur_length\"])\n",
    "filtered_valid_data = filtered_valid_data.remove_columns([\"cur_length\"])\n",
    "filtered_combined_datasets = DatasetDict({\"train\": filtered_train_data, \"valid\": filtered_valid_data})\n",
    "\n",
    "filtered_combined_datasets.save_to_disk(\"/vepfs/wcf/G/zecheng/data/SlimPajama-6B/dpo_data/hf_data_v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
