{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "content = auto_read_data(\"/data/zecheng/sunzc/result/dzc_res/longbench/Llama-3-8B-Instruct-80K-QLoRA/gov_report.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m|||---- Using logn scaling... ----|||\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n",
      "/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/zecheng/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 17\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 27\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 7\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 14\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 31\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 3\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 3\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 3\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 9\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 3\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 17\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 22\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 16\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 17\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 4\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 7\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 38\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 17\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 15\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 34\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 5\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 22\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 30\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 9\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 6\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 7\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 31\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 5\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 35\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 24\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs are there | Answer: 37\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 11\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs are there | Answer: 28\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs are there | Answer: 5\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 4\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 11\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 16\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 26\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 10\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 42\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs are there | Answer: 26\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 21\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 19\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 26\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 5\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 20\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 15\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 10\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many unique paragraphs there are | Answer: 17\n",
      "prediction: There are some paragraphs below sourced from Wikipedia Some of them may be duplicates Please carefully read these paragraphs and determine how many non-repeating | Answer: 28\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "import transformers\n",
    "import sys\n",
    "sys.path.append('/data/zecheng/Retrieval_Head/quick_eval')\n",
    "from utils import all_datasets, longbench_dataset_prompt, longbench_pred_length, load_model, DATASET2CATEGORY\n",
    "from chat import apply_chat_template\n",
    "\n",
    "model_path = '/data/zecheng/hf_models/longchat-7b-v1.5-32k'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "test_model = load_model(\n",
    "    model_name='llama2', model_path=model_path, \n",
    "    adapter_path='/nvme/zecheng/ckpt/simpo-llama2_fix/checkpoint-300/context_scaling', \n",
    "    max_training_length=16384, max_testing_length=32000, use_logn=True, device=torch.device('cuda:6'),\n",
    ")\n",
    "\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path, \n",
    "#     attn_implementation=\"flash_attention_2\", \n",
    "#     torch_dtype=torch.bfloat16, use_cache=True\n",
    "# ).to('cuda')\n",
    "\n",
    "dataset_name = 'passage_count'\n",
    "dir_path = '/data/zecheng/Retrieval_Head/quick_eval/data'\n",
    "test_data = auto_read_data(os.path.join(dir_path, f\"{dataset_name}.jsonl\"))\n",
    "\n",
    "PROMPT_TEMPLATE, PRED_LENGTH = longbench_dataset_prompt[dataset_name], longbench_pred_length[dataset_name]\n",
    "samples = [test_data[i] for i in range(150, len(test_data))]\n",
    "\n",
    "for sample in samples:\n",
    "    context, input_, answers = sample['context'], sample['input'], sample['answers']\n",
    "    prompt = PROMPT_TEMPLATE.format(input=input_, context=context)\n",
    "    prompt = apply_chat_template(\n",
    "        \"llama-2\", \n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        tokenizer=tokenizer,\n",
    "        add_generation_prompt=True,\n",
    "    ).raw\n",
    "\n",
    "    textual_input = tokenizer(prompt, return_tensors=\"pt\").to(test_model.device).input_ids\n",
    "\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    eos_token_id.append(tokenizer.encode(\"\\n\", add_special_tokens=False)[-1])\n",
    "\n",
    "    outputs = test_model.generate(\n",
    "        textual_input, \n",
    "        min_new_tokens=1,\n",
    "        max_new_tokens=PRED_LENGTH, \n",
    "        do_sample=None,\n",
    "        begin_suppress_tokens=eos_token_id,\n",
    "        eos_token_id=eos_token_id, \n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        )\n",
    "\n",
    "    pred_str = tokenizer.decode(outputs[0, textual_input.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(f'prediction: {pred_str} | Answer: {answers[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yanzhou']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.index('Kaiyuan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'passages.\\nPassage 1:\\nKaiyuan, Liaoning\\nKaiyuan (simplified C'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[140: 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on: ['qmsum_e.jsonl', 'qasper_e.jsonl', 'gov_report_e.jsonl', 'triviaqa_e.jsonl', '2wikimqa_e.jsonl', 'hotpotqa_e.jsonl', 'result.csv', 'trec.jsonl', 'multi_news_e.jsonl', 'passage_count.jsonl', 'samsum_e.jsonl', 'result.json', 'musique.jsonl', 'multifieldqa_en.jsonl', 'narrativeqa.jsonl', 'lcc_e.jsonl', 'passage_retrieval_en.jsonl', 'repobench-p_e.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from fire import Fire\n",
    "from modelzipper.tutils import *\n",
    "from utils import *\n",
    "from metrics import (\n",
    "    qa_f1_score, rouge_score, classification_score, \n",
    "    retrieval_score, count_score, code_sim_score,\n",
    ")\n",
    "\n",
    "dataset2metric = {\n",
    "    \"narrativeqa\": qa_f1_score,\n",
    "    \"qasper_e\": qa_f1_score,\n",
    "    \"multifieldqa_en\": qa_f1_score,\n",
    "    \"hotpotqa_e\": qa_f1_score,\n",
    "    \"2wikimqa_e\": qa_f1_score,\n",
    "    \"musique\": qa_f1_score,\n",
    "    \"gov_report_e\": rouge_score,\n",
    "    \"multi_news_e\": rouge_score,\n",
    "    \"trec\": classification_score,\n",
    "    \"triviaqa_e\": qa_f1_score,\n",
    "    \"samsum_e\": rouge_score,\n",
    "    \"passage_retrieval_en\": retrieval_score,\n",
    "    \"passage_count\": count_score,\n",
    "    \"lcc_e\": code_sim_score,\n",
    "    \"repobench-p_e\": code_sim_score,\n",
    "    \"qmsum_e\": rouge_score,\n",
    "}\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type=str, default=None)\n",
    "    parser.add_argument('--e', action='store_true', help=\"Evaluate on LongBench-E\")\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "def scorer_e(dataset, predictions, answers, lengths, all_classes):\n",
    "    scores = dict()\n",
    "    total_score = 0.0\n",
    "    for (prediction, ground_truths, length) in zip(predictions, answers, lengths):\n",
    "        score = 0.\n",
    "        if dataset in [\"trec\", \"triviaqa\", \"samsum\", \"lsht\"]:\n",
    "            prediction = prediction.lstrip('\\n').split('\\n')[0]\n",
    "        for ground_truth in ground_truths:\n",
    "            score = max(score, dataset2metric[dataset](prediction, ground_truth, all_classes=all_classes))\n",
    "        total_score += score\n",
    "    for key in scores.keys():\n",
    "        scores[key] = round(100 * np.mean(scores[key]), 2)\n",
    "    scores[\"total_score\"] = round(100 * total_score / len(predictions), 2)\n",
    "    return scores\n",
    "\n",
    "def scorer(dataset, predictions, answers, all_classes):\n",
    "    total_score = 0.\n",
    "    for (prediction, ground_truths) in zip(predictions, answers):\n",
    "        score = 0.\n",
    "        if dataset in [\"trec\", \"triviaqa_e\", \"samsum_e\", \"lsht\"]:\n",
    "            prediction = prediction.lstrip('\\n').split('\\n')[0]\n",
    "        for ground_truth in ground_truths:\n",
    "            score = max(score, dataset2metric[dataset](prediction, ground_truth, all_classes=all_classes))\n",
    "        total_score += score\n",
    "    return round(100 * total_score / len(predictions), 2)\n",
    "\n",
    "\n",
    "pred_path = '/vepfs/wcf/G/zecheng/Retrieval_Head/quick_eval/longbench/llama-2-simpo'\n",
    "benchmark_dataset_path=None\n",
    "scores = dict()\n",
    "all_files = os.listdir(pred_path)\n",
    "print(\"Evaluating on:\", all_files)\n",
    "\n",
    "## extract all golden data classes\n",
    "data_classes = {}\n",
    "if benchmark_dataset_path is None:\n",
    "    benchmark_dataset_path = \"/vepfs/wcf/G/zecheng/Retrieval_Head/quick_eval/data\"\n",
    "all_benchmark_data_files = os.listdir(benchmark_dataset_path)\n",
    "for f in all_benchmark_data_files:\n",
    "    data_name = f.split('.')[0]\n",
    "    content = auto_read_data(os.path.join(benchmark_dataset_path, f))\n",
    "    data_classes[data_name] = content[0][\"all_classes\"]\n",
    "\n",
    "## read all predicted datasets\n",
    "for filename in all_files:\n",
    "    if not filename.endswith(\"jsonl\"):\n",
    "        continue\n",
    "    dataset_name = filename.split('.')[0]\n",
    "    pred_dataset_length, all_classes = longbench_pred_length[dataset_name], data_classes[dataset_name]\n",
    "    predictions, answers = [], []\n",
    "    \n",
    "    with open(os.path.join(pred_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            predictions.append(data[\"pred_str\"])\n",
    "            answers.append(data[\"answers\"])\n",
    "\n",
    "    if len(predictions) == 0: continue\n",
    "    score = scorer(dataset_name, predictions, answers, all_classes)\n",
    "    scores[dataset_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qmsum_e': 22.15,\n",
       " 'qasper_e': 22.11,\n",
       " 'gov_report_e': 30.84,\n",
       " 'triviaqa_e': 66.86,\n",
       " '2wikimqa_e': 20.13,\n",
       " 'hotpotqa_e': 28.93,\n",
       " 'trec': 63.0,\n",
       " 'multi_news_e': 24.03,\n",
       " 'passage_count': 1.0,\n",
       " 'samsum_e': 29.97,\n",
       " 'musique': 9.93,\n",
       " 'multifieldqa_en': 33.39,\n",
       " 'narrativeqa': 12.01,\n",
       " 'lcc_e': 34.73,\n",
       " 'passage_retrieval_en': 6.5,\n",
       " 'repobench-p_e': 29.17}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = scores\n",
    "\n",
    "# 将JSON数据转换为DataFrame\n",
    "df = pd.DataFrame({k: v for k, v in json_data.items()}, index=[0])\n",
    "\n",
    "# 定义新的列顺序\n",
    "columns = [\n",
    "    ['Single-Document QA', 'Single-Document QA', 'Single-Document QA', 'Single-Document QA',\n",
    "    'Multi-Document QA', 'Multi-Document QA', 'Multi-Document QA', 'Multi-Document QA',\n",
    "    'Summarization', 'Summarization', 'Summarization', 'Summarization',\n",
    "    'Few-shot Learning', 'Few-shot Learning', 'Few-shot Learning', 'Few-shot Learning',\n",
    "    'Synthetic Tasks', 'Synthetic Tasks', 'Synthetic Tasks',\n",
    "    'Code Completion', 'Code Completion', 'Code Completion',\n",
    "    'ALL Avg'],\n",
    "    ['qasper_e', 'multifieldqa_en', 'narrativeqa', 'Avg.',\n",
    "    'hotpotqa_e', '2wikimqa_e', 'musique', 'Avg.',\n",
    "    'gov_report_e', 'qmsum_e', 'multi_news_e', 'Avg.',\n",
    "    'trec', 'triviaqa_e', 'samsum_e', 'Avg.',\n",
    "    'passage_count', 'passage_retrieval_en', 'Avg.',\n",
    "    'lcc_e', 'repobench-p_e', 'Avg.',\n",
    "    '']\n",
    "]\n",
    "\n",
    "# 重新组织数据\n",
    "new_df = pd.DataFrame(columns=columns)\n",
    "for col_0, col in zip(columns[0], columns[1]):\n",
    "    if col in df.columns:\n",
    "        new_df[(col_0, col)] = df[col]\n",
    "    elif col == 'Avg.':\n",
    "        new_df[(col_0, col)] = np.nan\n",
    "    else:\n",
    "        new_df[(col_0, col)] = np.nan\n",
    "\n",
    "# 计算平均值\n",
    "for category in ['Single-Document QA', 'Multi-Document QA', 'Summarization', 'Few-shot Learning', 'Synthetic Tasks', 'Code Completion']:\n",
    "    mask = new_df.columns.get_level_values(0) == category\n",
    "    new_df.loc[:, (category, 'Avg.')] = new_df.loc[:, mask].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Single-Document QA</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Multi-Document QA</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Summarization</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Few-shot Learning</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Synthetic Tasks</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Code Completion</th>\n",
       "      <th>ALL Avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>qasper_e</th>\n",
       "      <th>multifieldqa_en</th>\n",
       "      <th>narrativeqa</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>hotpotqa_e</th>\n",
       "      <th>2wikimqa_e</th>\n",
       "      <th>musique</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>gov_report_e</th>\n",
       "      <th>qmsum_e</th>\n",
       "      <th>...</th>\n",
       "      <th>triviaqa_e</th>\n",
       "      <th>samsum_e</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>passage_count</th>\n",
       "      <th>passage_retrieval_en</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>lcc_e</th>\n",
       "      <th>repobench-p_e</th>\n",
       "      <th>Avg.</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.11</td>\n",
       "      <td>33.39</td>\n",
       "      <td>12.01</td>\n",
       "      <td>22.503333</td>\n",
       "      <td>28.93</td>\n",
       "      <td>20.13</td>\n",
       "      <td>9.93</td>\n",
       "      <td>19.663333</td>\n",
       "      <td>30.84</td>\n",
       "      <td>22.15</td>\n",
       "      <td>...</td>\n",
       "      <td>66.86</td>\n",
       "      <td>29.97</td>\n",
       "      <td>53.276667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>34.73</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Single-Document QA                                        Multi-Document QA  \\\n",
       "            qasper_e multifieldqa_en narrativeqa       Avg.        hotpotqa_e   \n",
       "0              22.11           33.39       12.01  22.503333             28.93   \n",
       "\n",
       "                                Summarization          ... Few-shot Learning  \\\n",
       "  2wikimqa_e musique       Avg.  gov_report_e qmsum_e  ...        triviaqa_e   \n",
       "0      20.13    9.93  19.663333         30.84   22.15  ...             66.86   \n",
       "\n",
       "                      Synthetic Tasks                             \\\n",
       "  samsum_e       Avg.   passage_count passage_retrieval_en  Avg.   \n",
       "0    29.97  53.276667             1.0                  6.5  3.75   \n",
       "\n",
       "  Code Completion                      ALL Avg  \n",
       "            lcc_e repobench-p_e   Avg.          \n",
       "0           34.73         29.17  31.95     NaN  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Single-Document QA</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Multi-Document QA</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Summarization</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Few-shot Learning</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Synthetic Tasks</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Code Completion</th>\n",
       "      <th>ALL Avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>qasper_e</th>\n",
       "      <th>multifieldqa_en</th>\n",
       "      <th>narrativeqa</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>hotpotqa_e</th>\n",
       "      <th>2wikimqa_e</th>\n",
       "      <th>musique</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>gov_report_e</th>\n",
       "      <th>qmsum_e</th>\n",
       "      <th>...</th>\n",
       "      <th>triviaqa_e</th>\n",
       "      <th>samsum_e</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>passage_count</th>\n",
       "      <th>passage_retrieval_en</th>\n",
       "      <th>Avg.</th>\n",
       "      <th>lcc_e</th>\n",
       "      <th>repobench-p_e</th>\n",
       "      <th>Avg.</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.11</td>\n",
       "      <td>33.39</td>\n",
       "      <td>12.01</td>\n",
       "      <td>22.503333</td>\n",
       "      <td>28.93</td>\n",
       "      <td>20.13</td>\n",
       "      <td>9.93</td>\n",
       "      <td>19.663333</td>\n",
       "      <td>30.84</td>\n",
       "      <td>22.15</td>\n",
       "      <td>...</td>\n",
       "      <td>66.86</td>\n",
       "      <td>29.97</td>\n",
       "      <td>53.276667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>34.73</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.95</td>\n",
       "      <td>26.136111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Single-Document QA                                        Multi-Document QA  \\\n",
       "            qasper_e multifieldqa_en narrativeqa       Avg.        hotpotqa_e   \n",
       "0              22.11           33.39       12.01  22.503333             28.93   \n",
       "\n",
       "                                Summarization          ... Few-shot Learning  \\\n",
       "  2wikimqa_e musique       Avg.  gov_report_e qmsum_e  ...        triviaqa_e   \n",
       "0      20.13    9.93  19.663333         30.84   22.15  ...             66.86   \n",
       "\n",
       "                      Synthetic Tasks                             \\\n",
       "  samsum_e       Avg.   passage_count passage_retrieval_en  Avg.   \n",
       "0    29.97  53.276667             1.0                  6.5  3.75   \n",
       "\n",
       "  Code Completion                         ALL Avg  \n",
       "            lcc_e repobench-p_e   Avg.             \n",
       "0           34.73         29.17  31.95  26.136111  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算总平均值\n",
    "new_df[('ALL Avg', '')] = new_df.loc[:, new_df.columns.get_level_values(1) == 'Avg.'].mean(axis=1)\n",
    "new_df\n",
    "# out_path = os.path.join(pred_path, \"result.csv\")\n",
    "# # 保存为 CSV 文件\n",
    "# new_df.to_csv(out_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zecheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
